<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Bioinnovation Meeting - 2017/09/05 - Alexej Gossmann</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/league.css" id="league">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Emoji -->
    <!-- See: https://afeld.github.io/emoji-css/ -->
    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!-- MathJax macros (inline math delimiters, new commands, etc.), more configuration under Reveal.initialize below -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ["$","$"] ],
          displayMath: [ ["$$","$$"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: { autoNumber: "AMS" },
          Macros: {
            subscript: ['_{#1}', 1],
            superscript: ['^{#1}', 1]
          }
        }
      });
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section data-markdown>
          <script type="text/template">
### Bioinnovation Program Meeting
### Alexej Gossmann
#### 2017/05/09
          </script>
        </section>

        <!-- FDR work -->
        <section data-transition="slide" data-background="#5F819D" data-background-transition="zoom">
          <h2>I. My project at the FDA</h2>
          <h4>Center for Devices and Radiological Health</h4>
          <h4>Office of Science and Engineering Laboratories</h4>
          <h3>Division of Imaging, Diagnostics, and Software Reliability</h3>
        </section>

        <section>
          <section data-transition="slide-in fade-out">
            <img data-src="./img/20170905-bioinnovation-meeting/test_data_reuse_1.png">
          </section>

          <section data-transition="fade-in fade-out">
            <img data-src="./img/20170905-bioinnovation-meeting/test_data_reuse_2.png">
          </section>

          <section data-transition="fade-in slide-out">
            <img data-src="./img/20170905-bioinnovation-meeting/test_data_reuse_3.png">
          </section>

          <section>
            <img data-src="./img/20170905-bioinnovation-meeting/Flowchart.png">
          </section>

          <section data-markdown>
            <script type="text/template">
### Performance assessment in adaptive machine learning with test data reuse

Repeated usage of the same test data inadvertently leads to:

* __Overly optimistic performance assessments <i class="em em-cold_sweat"></i>__ (sometimes substantially so).
* __Loss of generalization <i class="em em-scream"></i>:__ A machine learning system that performs much better on the available test cases than on the general population; i.e., overfitting to the test dataset.
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
<small>
## Possible solution: Differentially private access to test data

* Differential privacy is a mathematically rigorous definition of data privacy (see work of Cynthia Dwork and her collaborators).
* __Intuition:__ If the test dataset can be accessed only via a differentially private mechanism, then the machine learning algorithm will have *no way to extract information about individual dataset records, but will only learn characteristics of the population as a whole*. <i class="em em-point_right"></i> Algo will adapt to the underlying distribution rather than to records in the specific dataset. <i class="em em-smiley"></i>
* Under certain theoretical conditions this works even if the test dataset is reused thousands of times (Dwork et. al., Science, 2015). <i class="em em-thumbsup"></i>
* In practice the reported performance metrics are much more accurate even when the theoretical conditions are not met (our work). <i class="em em-smiley_cat"></i>

</small>
            </script>
          </section>

          <section data-markdown style="text-align: left">
            <script type="text/template">
<small>

Algorithm: __Thresholdout__$\subscript{\mathrm{AUC}}$

**Input:**
- Training dataset $S\subscript{\mathrm{train}}$ and test dataset $S\subscript{\mathrm{test}}$.
- Noise rate $\sigma$, budget $B$, threshold $T$.
- Set $\hat{T} \gets T + \gamma$ for $\gamma \sim \mathrm{Lap}(2\sigma)$, where $\mathrm{Lap}(2\sigma)$ denotes the Laplace distribution with mean 0 and scale parameter $2\sigma$.

**Query step:**

Given a function $\phi$ that assigns a score between $0$ and $1$ to each observation, **do**:
- If $B < 1$ output $\bot$ (i.e., the test data access budget is exhausted).
- Else sample $\xi \sim \mathrm{Lap}(\sigma)$, $\gamma \sim \mathrm{Lap}(2\sigma)$, and $\eta \sim \mathrm{Lap}(4\sigma)$:
  * If $\left\lvert \widehat{\mathrm{AUC}}\subscript{S\subscript{\mathrm{test}}}(\phi) - \widehat{\mathrm{AUC}}\subscript{S\subscript{\mathrm{train}}}(\phi) \right\rvert > \hat{T} + \eta$, output $\widehat{\mathrm{AUC}}\subscript{S\subscript{\mathrm{test}}}(\phi) + \xi$ and set $B \gets B - 1$ and $\hat{T} \gets T + \gamma$.
  * Otherwise output $\widehat{\mathrm{AUC}}\subscript{S\subscript{\mathrm{train}}}(\phi)$.

</small>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
            ![Some simulation results from the SPIE submission](./img/20170905-bioinnovation-meeting/naive_holdout_reuse_vs_thresholdout_AUC_landscape.png)

            This work has been submitted to [SPIE Medical Imaging 2018](https://spie.org/conferences-and-exhibitions/medical-imaging/conferences?SSO=1) for publication in the [Proceeding of SPIE](http://proceedings.spiedigitallibrary.org/conferenceproceedings.aspx) and a conference presentation.
            </script>
          </section>
        </section>

        <!-- Tulane work -->
        <section data-transition="slide" data-background="#A54242" data-background-transition="zoom">
          <h2>II. My Tulane work</h2>
          <h3>Certain methods for FDR control in sparse regression and sparse CCA</h3>
        </section>

        <!-- The model selection problem -->
        <section>
          <section data-markdown>
            <script type="text/template">
### The model selection problem

* The simplest example: <i class="em em-thumbsup"></i> linear model.
* $\mathbf{y}=X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$, where
  - $\mathbf{y}\in\mathbb{R}^n$ dependent variable (e.g., disease status/severity for $n$ <i class="em em-mask"></i>),
  - $X\in\mathbb{R}^{n\times p}$ explanatory variables (for each <i class="em em-mask"></i> record $p$ features: <i class="em em-smoking"></i>, <i class="em em-syringe"></i>, <i class="em em-pill"></i>, ...),
  - *Unknowns*: $\boldsymbol{\beta}\in\mathbb{R}^p$ (want to estimate), $\boldsymbol{\varepsilon}$ noise.
* __Prediction__: Find best predictions for $\mathbf{y}$.
* __Feature selection__: Find which $\beta_i$ are non-zero.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### ...in genomics and brain imaging

## ..why we care

<i class="em em-microscope"></i> + <i class="em em-moneybag"></i>

* Prediction of a disease phenotype based on a handful of features is needed for inexpensive diagnosis.
* Elimination of noisy or redundant features leads to more accurate prediction.
* "Data-generated hypotheses" lead to a better understanding of the underlying biology.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### ...in genomics and brain imaging

## ..challenges

* *<del>Possibly</del> Only slightly less often than always $n \ll p$.* <i class="em em-sweat"></i>
* Curse of dimensionality (when $n < p$). <i class="em em-cold_sweat"></i>
* Overfitting. <i class="em em-scream"></i>
* Underfitting. <i class="em em-scream_cat"></i>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### $\ell_1$ regularization (e.g. LASSO by Tibshirani, 1994)

$$\hat{\boldsymbol{\beta}} = \arg\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert\mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \lambda\lVert\mathbf{b}\rVert\subscript{1}$$

* Yields a sparse $\hat{\boldsymbol{\beta}}$.
* Computationally efficient and very useful in practice.
* Problem 1: unclear how to select $\lambda$.
* Problem 2: unclear how to do statistical inference on $\hat{\boldsymbol{\beta}}$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Multiple hypotheses testing perspective

<font size="4">

Alternatively, feature selection can be regarded as testing the $p$ hypotheses
$$H\subscript{i} : \beta\subscript{i} = 0, \quad i = 1,\ldots,p.$$

<ul>

<li> Denote $R := $ number of rejected hypotheses, and $V := $ number of false rejections (i.e., Type I errors). </li>
<li> *Family-wise error rate*:
$$\mathrm{FWER} = \mathbb{P}\left(\mathrm{At\,least\,one\,false\,rejection}\right) = \mathbb{P}(V \geq 1).$$
E.g. Bonferroni correction (60ies?):
$$\mathbb{P}(V \geq 1) \leq \mathbb{P}\left( \bigcup\subscript{i = 1}^{n} \\{ H\subscript{i} \,\mathrm{falsely\,rejected} \\} \right) \leq \sum\subscript{i = 1}^{n} \underbrace{\mathbb{P}\left( \\{ H\subscript{i} \,\mathrm{falsely\,rejected} \\} \right)}\subscript{\leq \alpha / n} \leq \alpha.$$
</li>
<li> *False discovery rate* ('95):
$$\mathrm{FDR} = \mathbb{E}\left(\frac{\mathrm{\\#False\,rejections}}{\mathrm{\\#Rejections}}\right) = \mathbb{E}\left( \frac{V}{\min\\{R, 1\\}} \right).$$
E.g. Benjamini-Hochberg:
    <ol>
    <li> Sort the p-values $p\subscript{(1)} \leq p\subscript{(2)} \leq \ldots \leq p\subscript{(n)}$. </li>
    <li> Find the largest $k$ such that $p\subscript{(k)} \leq \frac{k}{n} \alpha$. </li>
    <li> Reject the null hypothesis for all $H\subscript{(i)}$ for $i = 1, \ldots, k$. </li>
    </ol>
</li>

</font>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
## The model selection problem

![Tibshirani, 1996, citation count](./img/20170905-bioinnovation-meeting/Tibshirani1996.png?raw=true)
![Benjamini & Hochberg, 1995, citation count](./img/20170905-bioinnovation-meeting/Benjamini1995.png?raw=true)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Sorted L-One Penalized Estimation (SLOPE, Bogdan et. al., Annals Appl Stat, 2015)

$$\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}} = \mathrm{argmin}\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert \mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \sum\subscript{i=1}^p \lambda\subscript{i} |\mathbf{b}|\subscript{(i)},$$

where $\lambda\subscript{1} \geq \lambda\subscript{2} \geq \ldots \geq \lambda\subscript{p} \geq 0$; and $|b|\subscript{(1)} \geq |b|\subscript{(2)} \geq \ldots \geq |b|\subscript{(p)}$ denotes the order statistic of the magnitudes of the vector $\mathbf{b}\in\mathbb{R}^p$.

<i class="em em-satisfied"></i> Given $q\in(0,1)$, there is a procedure to choose $\boldsymbol{\lambda}$ s.t. $\mathrm{FDR}(\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}}) \leq q$,...
<i class="em em-weary"></i> *if the explanatory variables have very small pair-wise correlations*.
            </script>
          </section>
        </section>

        <!-- Group SLOPE -->
        <section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE Motivation

* Typically, genomic data are highly correlated.
* Often the data can be subdivided into groups with possibly a high within group correlation but a low between group correlation. <del>(Oh really?)</del>
* _In case of biomedical data available prior knowledge often provides grouping structures naturally_. E.g., Genomic data: genes or genetic pathways; brain MRI data: anatomical atlases of brain regions; etc.

<i class="em em-thumbsup"></i> Select or drop entire groups rather than individual variables. Redefine FDR w.r.t. groups (**gFDR**).
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Model Formulation

* Let $X\in\mathbb{R}^{n\times p}$, $\boldsymbol{\beta}\in\mathbb{R}^p$, $\boldsymbol{\varepsilon}\sim\mathrm{N}(0, \sigma\subscript{\varepsilon}^2 I)$. 

* The predictor variables $\boldsymbol{\beta}$ are divided into $J$ groups of sizes $p_1, p_2, \cdots, p_J$, i.e. $\boldsymbol{\beta} = (\boldsymbol{\beta}_1^T, \boldsymbol{\beta}_2^T, \ldots, \boldsymbol{\beta}_J^T)^T$ with $\boldsymbol{\beta}_i \in \mathbb{R}^{p_i}$. 

* $\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon} = \sum\subscript{i=1}^J X_i \boldsymbol{\beta}_i + \boldsymbol{\varepsilon}$. 
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE Model

#### Formulation 1 (Gossmann et. al. 2015)

$$\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \left\lVert\mathbf{y} - X\mathbf{b}\right\rVert\subscript{2}^2 + \sum\subscript{i=1}^J \lambda\subscript{i} \sqrt{p\subscript{(i)}}\left\lVert\mathbf{b}\subscript{(i)}\right\rVert\subscript{2},$$

where $\sqrt{p\subscript{(1)}}\left\lVert \mathbf{b}\subscript{(1)} \right\rVert\subscript{2} \geq \sqrt{p\subscript{(2)}}\left\lVert \mathbf{b}\subscript{(2)} \right\rVert\subscript{2} \geq \ldots \geq \sqrt{p\subscript{(J)}}\left\lVert \mathbf{b}\subscript{(J)} \right\rVert\subscript{2}$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE Model

#### Formulation 2 (Brzyski et. al. 2016)

$$\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \left\lVert\mathbf{y} - X\mathbf{b}\right\rVert\subscript{2}^2 + \sum\subscript{i=1}^J \lambda\subscript{i} \sqrt{p\subscript{(i)}}\left\lVert X\subscript{(i)} \mathbf{b}\subscript{(i)}\right\rVert\subscript{2},$$

where $\sqrt{p\subscript{(1)}}\left\lVert  X\subscript{(1)} \mathbf{b}\subscript{(1)} \right\rVert\subscript{2} \geq \sqrt{p\subscript{(2)}}\left\lVert  X\subscript{(2)} \mathbf{b}\subscript{(2)} \right\rVert\subscript{2} \geq \ldots \geq \sqrt{p\subscript{(J)}}\left\lVert  X\subscript{(J)} \mathbf{b}\subscript{(J)} \right\rVert\subscript{2}$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE

* Given a user-specified $q \in (0, 1)$, we came up with a procedure to select $\boldsymbol{\lambda}$, such that we get $\mathrm{gFDR} \leq q$,
if any two variables *from different groups* are nearly uncorrelated (Brzyski, Gossmann, et. al., 2016; Gossmann et. al., 2016).
* The method was applied to DNA sequence data from the Framingham Heart Study, in order to predict bone mineral density and identify genes that influence it (Gossmann et. al., 2016).
            </script>
          </section>

          <section>
            <font size="5">
            <h2>Group SLOPE References</h2>
            <ol>
<li><span id="C-gossmann2015">Gossmann, A., Cao, S., &amp; Wang, Y.-P. (2015). Identification of Significant Genetic Variants via SLOPE, and Its Extension to Group SLOPE. In <i>Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics</i>, ACM BCB â€™15.</span> DOI: <a href="http://dx.doi.org/10.1145/2808719.2808743">10.1145/2808719.2808743</a>. </li>

<li><span id="P-gossmann2016">Gossmann, A., Cao., S., Brzyski, D., Zhao, L.-J., Deng, H.-W., &amp; Wang, Y.-P. (2016). A sparse regression method for group-wise feature selection with false discovery rate control.</span> <i>(Under review in IEEE/TCBB)</i> </li>

<li><span id="P-brzyski2016">Brzyski, D., Gossmann, A., Su, W., &amp; Bogdan, M. (2016). Group SLOPE &mdash; adaptive selection of groups of predictors.</span>
 <a href="http://arxiv.org/abs/1610.04960">arXiv:1610.04960</a>. <i>(Under review in JASA)</i>  </li>

<li>R packages:</li>
  <ul>
    <li> <a href=https://cran.r-project.org/package=grpSLOPE>cran.r-project.org/package=grpSLOPE</a> </li>
    <li> <a href=https://github.com/agisga/grpSLOPEMC>github.com/agisga/grpSLOPEMC</a> </li>
  </ul>
            </ol>
            </font>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
### Sparse canonical correlation analysis
![Sparse CCA illustration](./img/20170905-bioinnovation-meeting/SCCA.png)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Canonical correlation analysis

Let $x_1, \dots, x_n \in \mathbb{R}^p$ be independent $\mathcal{N}(0, \Sigma_X)$, $y_1, \dots, y_n \in \mathbb{R}^q$ be independent $\mathcal{N}(0, \Sigma_Y)$, $\mathrm{Cov}(x_k, y_k) = \Sigma\subscript{XY} \in \mathbb{R}^{p\times q}$ for all $k\in\left\\{ 1,\dots,n \right\\}$, and that $\mathrm{Cov}(x_k, y_j) = 0$ whenever $k \neq j$.

$$
X :=
\begin{bmatrix}
  x_1^T \\\\
  x_2^T \\\\
  \vdots \\\\
  x_n^T
\end{bmatrix} \in \mathbb{R}^{n\times p},
\quad
Y :=
\begin{bmatrix}
  y_1^T \\\\
  y_2^T \\\\
  \vdots \\\\
  y_n^T
\end{bmatrix} \in \mathbb{R}^{n\times q}.
$$
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Classical canonical correlation analysis

$$
\mathrm{maximize}\subscript{u\in\mathbb{R}^p, v\in\mathbb{R}^q} \widehat{\mathrm{Cov}}(Xu, Yv) = \frac{1}{n} u^T X^T Y v,
$$

$$
\mathrm{subject\,to} \quad \widehat{\mathrm{Var}}(Xu) = 1, \widehat{\mathrm{Var}}(Yv) = 1.
$$

* Due to Hotelling, 1936.
* The solution is called first pair of canonical vectors.
* Subsequent pairs of canonical vectors are restricted to be uncorrelated with the previous ones.
* The problem is degenerate if $n \leq \mathrm{max}\left( p, q \right)$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Sparse CCA

<small>

* Sparsity in the CCA solution can be achieved by utilizing penalty terms such as the $\ell_1$-norm. Unique solution even when $p_X, p_Y \gg n$.
* Witten et. al. (2009):
$$
\begin{eqnarray}
&\mathrm{maximize}\subscript{u\in\mathbb{R}^p, v\in\mathbb{R}^q} \frac{1}{n} u^T X^T Y v, \nonumber \\\\
&\mathrm{subject\,to} \quad \lVert u \rVert_2^2 \leq 1, \lVert v \rVert_2^2 \leq 1, \nonumber \\\\
&\mathrm{and} \quad \lVert u \rVert_1 \leq c_1, \lVert v \rVert_1 \leq c_2. \nonumber
\end{eqnarray}
$$
* Selection of $c_1$ and $c_2$ remains a challenging problem.
* Higher-order pairs of canonical vectors can be found by applying sparse CCA to a residual matrix, obtained from $X^T Y$ and the previously found canonical variates.

</small>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Defining false discovery rate (FDR) for sparse CCA

* Consider the FDR in $u$ and in $v$ separately.
* Consider $p_X$ hypotheses tests $H_i : u_i = 0$.
* The null hypothesis $H_i$ is true if the $i$th feature in $X$ is uncorrelated with all features in $Y$, i.e., if
$$(\forall j\in\left\\{ 1, 2, \dots, p_Y \right\\}) : \rho^{XY}\subscript{i, j} = 0.$$
* Let $R\subscript{\hat{u}}$ be the number of the rejected $H_i$, and $V\subscript{\hat{u}}$ the number of false rejections (i.e., when $\hat{u}_i \neq 0$ but $\rho^{XY}\subscript{i, j} = 0$ for all $j$).
* Define the false discovery rate in $u$ as
$$\mathrm{FDR}(\hat{u}) := \mathbb{E}\left( \frac{V\subscript{\hat{u}}}{\max\left\\{ R\subscript{\hat{u}}, 1 \right\\}} \right).$$
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### The FDR-corrected sparse CCA procedure
<small>

1. Divide each of $X$ and $Y$ into two subsets of sizes $n_0$ and $n_1$:
$$
X = \begin{bmatrix} X^{(0)} \\\\ X^{(1)} \end{bmatrix}
\quad \text{and} \quad
Y = \begin{bmatrix} Y^{(0)} \\\\ Y^{(1)} \end{bmatrix}.
$$
2. Obtain preliminary sparse CCA estimates $\hat{u}^{(0)}$ and $\hat{v}^{(0)}$ on $X^{(0)}$ and $Y^{(0)}$. Additionally, use $X^{(0)}$ and $Y^{(0)}$ to obtain $\widehat{\Sigma}^{(0)}$, the ML estimate of $\mathrm{Cov}\left(\begin{bmatrix} X & Y \end{bmatrix}\right)$.
3. Obtain p-values using the asymptotic approximation (under the null)
$$\left(\frac{1}{\sqrt{n}} \left( X^{(1)} \right)^T Y^{(1)} \hat{v}^{(0)} \middle| \Sigma = \widehat{\Sigma}^{(0)} \right) \sim \mathcal{N}\left( 0, \widehat{\Omega}^{(0)} \right),$$
where $\hat{\mu}^{(0)}$ and $\widehat{\Omega}^{(0)}$ are available in explicit form ($\hat{\mu}^{(0)} = 0$ under the null hypothesis).
4. Apply an FDR correcting procedure (such as BHq), and obtain the FDR-corrected estimates:
$$
\begin{eqnarray}
\hat{u}^{(1)}_i &:= \begin{cases}
  \left(X^T Y \hat{v}^{(0)}\right)_i, &\quad\text{for any rejected }H_i^{(u)},\\\\
  0, &\quad\text{otherwise}.
\end{cases}\\\\
\hat{v}^{(1)}_j &:= \begin{cases}
  \left(Y^T X \hat{u}^{(0)}\right)_j, &\quad\text{for any rejected }H_j^{(v)},\\\\
  0, &\quad\text{otherwise}.
\end{cases}
\end{eqnarray}
$$

</small>
            </script>
          </section>

          <section>
            <h3>Preprint</h3>
<ul>
  <li>Gossmann, A., Zille, P., Calhoun, V., &amp; Wang, Y.-P. (2017). FDR-Corrected Sparse Canonical Correlation Analysis with Applications to Imaging Genomics. <a href="http://arxiv.org/abs/1705.04312">arXiv:1705.04312</a> [<a href="http://arxiv.org/pdf/1705.04312">pdf</a>] <i>(under review in IEEE/TMI)</i>
  </li>

  <li>Associated code: <a href="https://github.com/agisga/FDRcorrectedSCCA">https://github.com/agisga/FDRcorrectedSCCA</a> </li>
</ul>
          </section>

          <section style="line-height:16px">
            <h4>Application to imaging genomics</h4>

<img data-src="./img/20170905-bioinnovation-meeting/voxels_vs_genes.png?raw=true">

<br>
<font size = "3"> <i>Data:</i> The Philadelphia Neurodevelopmental Cohort (PNC) is a large-scale collaborative study between the Brain Behaviour Laboratory at the University of Pennsylvania and the Children's Hospital of Philadelphia. It contains, among other modalities, a fractal $n$-back fMRI task, and SNP arrays for over 900 adolescents. </font>
          </section>

          <section data-markdown>
            <script type="text/template">
### Imaging genomics results

* We group the selected voxels using the region of interest (ROI) definitions of the AAL parcellation. The findings correspond to the *middle occipital gyri*, *left and right calcarine sulcus*, and *left cuneus* (3 voxels). Similar brain regions have been found in other fMRI studies of working memory.
* A literature search confirmed that a majority of the identified genes (at least 34 out of the 65) have been previously associated with various aspects of human cognitive function.
            </script>
          </section>
        </section>


        <section data-markdown>
          <script type="text/template">
# The End

# Thank you
          </script>
        </section>


      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        history: true,
        slideNumber: true,
        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // MathJax integration
        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'
        },

        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/math/math.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });

      Reveal.configure({ pdfMaxPagesPerSlide: 1 });
    </script>
  </body>
</html>
