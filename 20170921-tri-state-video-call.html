<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Estimation with FDR control - 2017/09/21 - Alexej Gossmann</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/league.css" id="league">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Emoji -->
    <!-- See: https://afeld.github.io/emoji-css/ -->
    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!-- MathJax macros (inline math delimiters, new commands, etc.), more configuration under Reveal.initialize below -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ["$","$"] ],
          displayMath: [ ["$$","$$"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: { autoNumber: "AMS" },
          Macros: {
            subscript: ['_{#1}', 1],
            superscript: ['^{#1}', 1]
          }
        }
      });
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section data-markdown>
          <script type="text/template">
### Controlling the false discovery rate in sparse and high-dimensional statistical methods, with applications in genomics and imaging
### Alexej Gossmann
### Tulane University
#### 2017/05/21
          </script>
        </section>

        <!-- The model selection problem -->
        <section>
          <section data-markdown>
            <script type="text/template">
## Background
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### The model selection problem

* The simplest example: <i class="em em-thumbsup"></i> linear model.
* $\mathbf{y}=X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$, where
  - $\mathbf{y}\in\mathbb{R}^n$ dependent variable (e.g., disease status/severity for $n$ <i class="em em-mask"></i>),
  - $X\in\mathbb{R}^{n\times p}$ explanatory variables (for each <i class="em em-mask"></i> record $p$ features: <i class="em em-smoking"></i>, <i class="em em-syringe"></i>, <i class="em em-pill"></i>, SNPs, fMRI, ...),
  - *Unknowns*: $\boldsymbol{\beta}\in\mathbb{R}^p$ (want to estimate), $\boldsymbol{\varepsilon}$ noise.
* __Prediction__: Find best predictions for $\mathbf{y}$.
* __Feature selection__: Find which $\beta_i$ are non-zero.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### ...in genomics and brain imaging

## ..why we care

<i class="em em-microscope"></i> + <i class="em em-moneybag"></i>

* Prediction of a disease phenotype based on a handful of features is needed for inexpensive diagnosis.
* Elimination of noisy or redundant features leads to more accurate prediction.
* "Data-generated hypotheses" lead to a better understanding of the underlying biology.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### ...in genomics and brain imaging

## ..challenges

* *<del>Possibly</del> Only slightly less often than always $n \ll p$.* <i class="em em-sweat"></i>
* Curse of dimensionality (when $n < p$). <i class="em em-cold_sweat"></i>
* Overfitting. <i class="em em-scream"></i>
* Underfitting. <i class="em em-scream_cat"></i>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### $\ell_1$ regularization (e.g. LASSO by Tibshirani, 1994)

$$\hat{\boldsymbol{\beta}} = \arg\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert\mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \lambda\lVert\mathbf{b}\rVert\subscript{1}$$

* Yields a sparse $\hat{\boldsymbol{\beta}}$.
* Computationally efficient and very useful in practice.
* Problem 1: unclear how to select $\lambda$.
* Problem 2: unclear how to do statistical inference on $\hat{\boldsymbol{\beta}}$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Multiple hypotheses testing perspective

<font size="4">

Alternatively, feature selection can be regarded as testing the $p$ hypotheses
$$H\subscript{i} : \beta\subscript{i} = 0, \quad i = 1,\ldots,p.$$

<ul>

<li> Denote $R := $ number of rejected hypotheses, and $V := $ number of false rejections (i.e., Type I errors). </li>
<li> *Family-wise error rate*:
$$\mathrm{FWER} = \mathbb{P}\left(\mathrm{At\,least\,one\,false\,rejection}\right) = \mathbb{P}(V \geq 1).$$
E.g. Bonferroni correction (60ies?):
$$\mathbb{P}(V \geq 1) \leq \mathbb{P}\left( \bigcup\subscript{i = 1}^{n} \\{ H\subscript{i} \,\mathrm{falsely\,rejected} \\} \right) \leq \sum\subscript{i = 1}^{n} \underbrace{\mathbb{P}\left( \\{ H\subscript{i} \,\mathrm{falsely\,rejected} \\} \right)}\subscript{\leq \alpha / n} \leq \alpha.$$
</li>
<li> *False discovery rate* ('95):
$$\mathrm{FDR} = \mathbb{E}\left(\frac{\mathrm{\\#False\,rejections}}{\mathrm{\\#Rejections}}\right) = \mathbb{E}\left( \frac{V}{\min\\{R, 1\\}} \right).$$
E.g. Benjamini-Hochberg:
    <ol>
    <li> Sort the p-values $p\subscript{(1)} \leq p\subscript{(2)} \leq \ldots \leq p\subscript{(n)}$. </li>
    <li> Find the largest $k$ such that $p\subscript{(k)} \leq \frac{k}{n} \alpha$. </li>
    <li> Reject the null hypothesis for all $H\subscript{(i)}$ for $i = 1, \ldots, k$. </li>
    </ol>
</li>

</font>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
## The model selection problem

![Tibshirani, 1996, citation count](./img/20170921-tri-state/Tibshirani1996.png?raw=true)
![Benjamini & Hochberg, 1995, citation count](./img/20170921-tri-state/Benjamini1995.png?raw=true)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Sorted L-One Penalized Estimation (SLOPE, Bogdan et. al., Annals Appl Stat, 2015)

$$\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}} = \mathrm{argmin}\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert \mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \sum\subscript{i=1}^p \lambda\subscript{i} |\mathbf{b}|\subscript{(i)},$$

where $\lambda\subscript{1} \geq \lambda\subscript{2} \geq \ldots \geq \lambda\subscript{p} \geq 0$; and $|b|\subscript{(1)} \geq |b|\subscript{(2)} \geq \ldots \geq |b|\subscript{(p)}$ denotes the order statistic of the magnitudes of the vector $\mathbf{b}\in\mathbb{R}^p$.

$\leadsto$ Given $q\in(0,1)$, there is a procedure to choose $\boldsymbol{\lambda}$ s.t. $\mathrm{FDR}(\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}}) \leq q$ is guaranteed,
*...if the explanatory variables have very small pair-wise correlations*.
            </script>
          </section>
        </section>

        <!-- Group SLOPE -->
        <section>

          <section data-markdown>
            <script type="text/template">
## Group SLOPE
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE Motivation

* Typically, genomic data are highly correlated.
* Often the data can be subdivided into groups with possibly a high within group correlation but a low between group correlation. <del>(Oh really?)</del>
* _In case of biomedical data available prior knowledge often provides grouping structures naturally_. E.g., Genomic data: genes or genetic pathways; brain MRI data: anatomical atlases of brain regions; etc.

<i class="em em-thumbsup"></i> Select or drop entire groups rather than individual variables. Redefine FDR w.r.t. groups (**gFDR**).
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Model Formulation

* Let $X\in\mathbb{R}^{n\times p}$, $\boldsymbol{\beta}\in\mathbb{R}^p$, $\boldsymbol{\varepsilon}\sim\mathrm{N}(0, \sigma\subscript{\varepsilon}^2 I)$.

* The predictor variables $\boldsymbol{\beta}$ are divided into $J$ groups of sizes $p_1, p_2, \cdots, p_J$, i.e. $\boldsymbol{\beta} = (\boldsymbol{\beta}_1^T, \boldsymbol{\beta}_2^T, \ldots, \boldsymbol{\beta}_J^T)^T$ with $\boldsymbol{\beta}_i \in \mathbb{R}^{p_i}$.

* $\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon} = \sum\subscript{i=1}^J X_i \boldsymbol{\beta}_i + \boldsymbol{\varepsilon}$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE Model

#### Formulation 1 (Gossmann et. al. 2015)

$$\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \left\lVert\mathbf{y} - X\mathbf{b}\right\rVert\subscript{2}^2 + \sum\subscript{i=1}^J \lambda\subscript{i} \sqrt{p\subscript{(i)}}\left\lVert\mathbf{b}\subscript{(i)}\right\rVert\subscript{2},$$

where $\sqrt{p\subscript{(1)}}\left\lVert \mathbf{b}\subscript{(1)} \right\rVert\subscript{2} \geq \sqrt{p\subscript{(2)}}\left\lVert \mathbf{b}\subscript{(2)} \right\rVert\subscript{2} \geq \ldots \geq \sqrt{p\subscript{(J)}}\left\lVert \mathbf{b}\subscript{(J)} \right\rVert\subscript{2}$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE Model

#### Formulation 2 (Brzyski et. al. 2016)

$$\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \left\lVert\mathbf{y} - X\mathbf{b}\right\rVert\subscript{2}^2 + \sum\subscript{i=1}^J \lambda\subscript{i} \sqrt{p\subscript{(i)}}\left\lVert X\subscript{(i)} \mathbf{b}\subscript{(i)}\right\rVert\subscript{2},$$

where $\sqrt{p\subscript{(1)}}\left\lVert  X\subscript{(1)} \mathbf{b}\subscript{(1)} \right\rVert\subscript{2} \geq \sqrt{p\subscript{(2)}}\left\lVert  X\subscript{(2)} \mathbf{b}\subscript{(2)} \right\rVert\subscript{2} \geq \ldots \geq \sqrt{p\subscript{(J)}}\left\lVert  X\subscript{(J)} \mathbf{b}\subscript{(J)} \right\rVert\subscript{2}$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE - Theoretical results

* Given a user-specified $q \in (0, 1)$, we came up with several procedures to select $\boldsymbol{\lambda}$, such that we get $\mathrm{gFDR} \leq q$,
if any two variables *from different groups* are nearly uncorrelated (Brzyski, Gossmann, et. al., 2016; Gossmann et. al., 2016).
* Under certain condition Group SLOPE enjoys some appealing estimation properties (asymptotically minimax, see Brzyski, Gossmann et. al., 2016).
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Simulation with orthogonal groups

![Group SLOPE orthogonal groups](./img/20170921-tri-state/gFDR_ortho.png)

<font size="4">$X \in\mathbb{R}^{5000\times 5000}$; signal strength $\approx$ expected max. noise; 300 repitions at each sparsity level.</font>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Simulation in non-orthogonal case

![Chromosome 22 SNPs, simulated phenotype](./img/20170921-tri-state/gFDR_non-ortho.png)

<font size="4"> $X \in \mathbb{R}^{8915\times 5976}$ contains real SNP data; 726 groups (mean size $8.23$, median size 1); between-group corr. < 0.3; *simulated response* $\mathbf{y} = X\boldsymbol{\beta} + \mathbf{z},$ where $\mathbf{z} \sim \mathcal{N}(0, I)$. </font>
            </script>
          </section>
        </section>

        <!-- FHS -->
        <section>
          <section data-markdown>
            <script type="text/template">
## Group SLOPE application example
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
## Framingham Cohort

### Preprocessing

* Exclusion of individuals or SNPs with more than 10% missing genotypes.
* Genotype imputation via IMPUTE2 based on the filtered data.
* The resulting preprocessed dataset consists of 8915 subjects’ genotype data with 476907 annotated SNPs.
* Only 1771 subjects have corresponding spine BMD measurements.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
## Framingham Cohort

### Clustering

* Only 1771 subjects have corresponding spine BMD measurements.
* We use the remaining over 7000 subjects to cluster the SNPs.
* Hierarchical clustering with an upper bound of 100 on cluster size, such that SNPs from different clusters have correlation < 0.3.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
## Framingham Cohort

### Variable screening (p-value thresholding)

1. Obtain a p-value for each group of SPNs using an ordinary linear model and the F-test.
2. Retain only groups with p-value < 0.1.

$\Rightarrow$ Resulting $X$ has dimensions $1771 \times 117933$, and consists of 6403 groups with average size equal to 18.42 (median size 2).
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE results

* 40 SNPs were selected by Group SLOPE with target gFDR $q = 0.1$, and mapped to nearby genes.
* 15 genes have been found in previous studies to be associated with:
  - BMD (SMOC1, RPS6KA5, FGFR2, GAA, SCN1A, RAB5A, SOX1, and A2BP1),
  - osteoarthritis (A2BP1, ADAM12, MATN1),
  - lumbar disc herniation (KIAA1217),
  - osteopetrosis (VAV3),
  - biology of osteoclasts, osteoblasts and osteogenesis (VAV3, SLC7A7, ADAM12, PPARD, FGFR2, PTPRU, SMOC1).
            </script>
          </section>
        </section>

        <section>
          <font size="5">
          <h2>Group SLOPE References</h2>
          <ol>
<li><span id="C-gossmann2015">Gossmann, A., Cao, S., &amp; Wang, Y.-P. (2015). Identification of Significant Genetic Variants via SLOPE, and Its Extension to Group SLOPE. In <i>Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics</i>, ACM BCB ’15.</span> DOI: <a href="http://dx.doi.org/10.1145/2808719.2808743">10.1145/2808719.2808743</a>. </li>

<li><span id="P-gossmann2016">Gossmann, A., Cao., S., Brzyski, D., Zhao, L.-J., Deng, H.-W., &amp; Wang, Y.-P. (2016). A sparse regression method for group-wise feature selection with false discovery rate control.</span> <i>(Under review in IEEE/TCBB)</i> </li>

<li><span id="P-brzyski2016">Brzyski, D., Gossmann, A., Su, W., &amp; Bogdan, M. (2016). Group SLOPE &mdash; adaptive selection of groups of predictors.</span>
 <a href="http://arxiv.org/abs/1610.04960">arXiv:1610.04960</a>. <i>(Under review in JASA)</i>  </li>

<li>R packages:</li>
  <ul>
    <li> <a href=https://cran.r-project.org/package=grpSLOPE>cran.r-project.org/package=grpSLOPE</a> </li>
    <li> <a href=https://github.com/agisga/grpSLOPEMC>github.com/agisga/grpSLOPEMC</a> </li>
  </ul>
          </ol>
          </font>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
## Controlling FDR in sparse canonical correlation analysis
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Sparse canonical correlation analysis
![Sparse CCA illustration](./img/20170921-tri-state/SCCA.png)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Canonical correlation analysis

Let $x_1, \dots, x_n \in \mathbb{R}^p$ be independent $\mathcal{N}(0, \Sigma_X)$, $y_1, \dots, y_n \in \mathbb{R}^q$ be independent $\mathcal{N}(0, \Sigma_Y)$, $\mathrm{Cov}(x_k, y_k) = \Sigma\subscript{XY} \in \mathbb{R}^{p\times q}$ for all $k\in\left\\{ 1,\dots,n \right\\}$, and that $\mathrm{Cov}(x_k, y_j) = 0$ whenever $k \neq j$.

$$
X :=
\begin{bmatrix}
  x_1^T \\\\
  x_2^T \\\\
  \vdots \\\\
  x_n^T
\end{bmatrix} \in \mathbb{R}^{n\times p},
\quad
Y :=
\begin{bmatrix}
  y_1^T \\\\
  y_2^T \\\\
  \vdots \\\\
  y_n^T
\end{bmatrix} \in \mathbb{R}^{n\times q}.
$$
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Classical canonical correlation analysis

$$
\mathrm{maximize}\subscript{u\in\mathbb{R}^p, v\in\mathbb{R}^q} \widehat{\mathrm{Cov}}(Xu, Yv) = \frac{1}{n} u^T X^T Y v,
$$

$$
\mathrm{subject\,to} \quad \widehat{\mathrm{Var}}(Xu) = 1, \widehat{\mathrm{Var}}(Yv) = 1.
$$

* Due to Hotelling, 1936.
* The solution is called first pair of canonical vectors.
* Subsequent pairs of canonical vectors are restricted to be uncorrelated with the previous ones.
* The problem is degenerate if $n \leq \mathrm{max}\left( p, q \right)$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Sparse CCA

<small>

* Sparsity in the CCA solution can be achieved by utilizing penalty terms such as the $\ell_1$-norm. Unique solution even when $p_X, p_Y \gg n$.
* Witten et. al. (2009):
$$
\begin{eqnarray}
&\mathrm{maximize}\subscript{u\in\mathbb{R}^p, v\in\mathbb{R}^q} \frac{1}{n} u^T X^T Y v, \nonumber \\\\
&\mathrm{subject\,to} \quad \lVert u \rVert_2^2 \leq 1, \lVert v \rVert_2^2 \leq 1, \nonumber \\\\
&\mathrm{and} \quad \lVert u \rVert_1 \leq c_1, \lVert v \rVert_1 \leq c_2. \nonumber
\end{eqnarray}
$$
* _Selection of the sparsity parameters remains a challenging problem_ (current options: cross-validation, AIC, permutation-based).
* Higher-order pairs of canonical vectors can be found by applying sparse CCA to a residual matrix, obtained from $X^T Y$ and the previously found canonical variates.

</small>
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
## FDR-correction for sparse CCA
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Defining false discovery rate (FDR) for sparse CCA

* Consider the FDR in $u$ and in $v$ separately.
* Consider $p_X$ hypotheses tests $H_i : u_i = 0$.
* The null hypothesis $H_i$ is true if the $i$th feature in $X$ is uncorrelated with all features in $Y$, i.e., if
$$(\forall j\in\left\\{ 1, 2, \dots, p_Y \right\\}) : \rho^{XY}\subscript{i, j} = 0.$$
* Let $R\subscript{\hat{u}}$ be the number of the rejected $H_i$, and $V\subscript{\hat{u}}$ the number of false rejections (i.e., when $\hat{u}_i \neq 0$ but $\rho^{XY}\subscript{i, j} = 0$ for all $j$).
* Define the false discovery rate in $u$ as
$$\mathrm{FDR}(\hat{u}) := \mathbb{E}\left( \frac{V\subscript{\hat{u}}}{\max\left\\{ R\subscript{\hat{u}}, 1 \right\\}} \right).$$
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### FDR-corrected sparse CCA

* In the classical CCA problem $u \propto X^T Y v$ (b/c SVD), and $v \propto Y^T X u$.
* Thus, the above tests are equivalent to
$$H_i : \left(X^TYv\right)_i = 0, \quad i\in\left\\{ 1, 2, \dots, p_X \right\\}.$$
* This motivates an FDR-correcting approach:
  1. Obtain initial estimates $\hat{u}^{(0)}$ and $\hat{v}^{(0)}$
  2. Then in order to determine which entries of $u$ and $v$ are truly non-zero, test null hypotheses of the form
  $$
  \begin{eqnarray}
  &\mathrm{H}^{(u)}_i : \left(X^T Y \hat{v}^{(0)}\right)_i = 0, \quad i = 1, 2, \ldots, p_X,\\\\
  &\mathrm{H}^{(v)}_j : \left(Y^T X \hat{u}^{(0)}\right)_j = 0 \quad j = 1, 2, \ldots, p_Y.
  \end{eqnarray}
  $$
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
<small>

### The FDR-corrected sparse CCA procedure

1. Divide each of $X$ and $Y$ into two subsets of sizes $n_0$ and $n_1$:
$$
X = \begin{bmatrix} X^{(0)} \\\\ X^{(1)} \end{bmatrix}
\quad \text{and} \quad
Y = \begin{bmatrix} Y^{(0)} \\\\ Y^{(1)} \end{bmatrix}.
$$
2. Obtain preliminary sparse CCA estimates $\hat{u}^{(0)}$ and $\hat{v}^{(0)}$ on $X^{(0)}$ and $Y^{(0)}$. Additionally, use $X^{(0)}$ and $Y^{(0)}$ to obtain $\widehat{\Sigma}^{(0)}$, the ML estimate of $\mathrm{Cov}\left(\begin{bmatrix} X & Y \end{bmatrix}\right)$.
3. Obtain p-values using the asymptotic approximation (under the null)
$$\left(\frac{1}{\sqrt{n}} \left( X^{(1)} \right)^T Y^{(1)} \hat{v}^{(0)} \middle| \Sigma = \widehat{\Sigma}^{(0)} \right) \sim \mathcal{N}\left( 0, \widehat{\Omega}^{(0)} \right),$$
where $\hat{\mu}^{(0)}$ and $\widehat{\Omega}^{(0)}$ are available in explicit form ($\hat{\mu}^{(0)} = 0$ under the null hypothesis).
4. Apply an FDR correcting procedure (such as BHq), and obtain the FDR-corrected estimates:
$$
\begin{eqnarray}
\hat{u}^{(1)}_i &:= \begin{cases}
  \left(X^T Y \hat{v}^{(0)}\right)_i, &\quad\text{for any rejected }H_i^{(u)},\\\\
  0, &\quad\text{otherwise}.
\end{cases}\\\\
\hat{v}^{(1)}_j &:= \begin{cases}
  \left(Y^T X \hat{u}^{(0)}\right)_j, &\quad\text{for any rejected }H_j^{(v)},\\\\
  0, &\quad\text{otherwise}.
\end{cases}
\end{eqnarray}
$$

</small>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
## Simulation results

1. We show simulation results under **Gaussian scenarios**, in order to verify that the proposed procedure indeed controls the FDR under the assumptions that its derivation relies on.
2. We show simulation studies evaluating the performance on **non-Gaussian data**, which are generated based on real single-nucleotide polymorphism (SNP) data.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Simulation study with Gaussian data

The data $\begin{bmatrix} X & Y \end{bmatrix}$ are generated from $\mathcal{N}(0, \Sigma)$, where $\Sigma$ is blockwise constant.

![Covariance matrix](./img/20170921-tri-state/Sigma_constant.png?raw=true)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Simulation study with Gaussian data

![Estimated FDR(v) on Gaussian data](./img/20170921-tri-state/constant_combined.png?raw=true)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Simulation study with Gaussian data

![Estimated TP(v) on Gaussian data](./img/20170921-tri-state/constant_TP_combined.png?raw=true)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Simulation study with non-Gaussian data (investigating robustness to distributional assumptions)

![Estimated FDR on Non-Gaussian data](./img/20170921-tri-state/FDR_combined.png?raw=true)
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
## FDR-Corrected SCCA Application to Imaging Genomics
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Application to imaging genomics

#### Data

*The Philadelphia Neurodevelopmental Cohort (PNC)* is a large-scale collaborative study between the Brain Behaviour Laboratory at the University of Pennsylvania and the Children's Hospital of Philadelphia. It contains, among other modalities, a fractal $n$-back fMRI task, and SNP arrays for over 900 adolescents.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Data

* The fractal $n$-back fMRI data were pre-processed using SPM12. Stimulus-on versus stimulus-off contrast maps were extracted for analysis. After discarding voxels with more than 1% missing data, the dataset consists of $85,796$ voxels.
* The SNP dataset contains $98,804$ SNPs (after pre-processing). PCA was performed within each gene to reduce dimensionality, resulting in $60,372$ genomic features.
* Our goal is to identify the essential regions of cross-correlation between the brain voxels and the genomic features.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Results

![PNC analysis results](./img/20170921-tri-state/voxels_vs_genes.png?raw=true)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Results

* We group the selected voxels using the ROI definitions of the AAL parcellation. The most significant findings correspond to the *middle occipital gyri* (13 voxels). Additional selected voxels lie in the *left and right calcarine sulcus* (158 voxels), and *left cuneus* (3 voxels). Similar brain regions have been found in other fMRI studies of working memory.
* A literature search confirmed that a majority of the identified genes (at least 34 out of the 65) have been previously associated with various aspects of human cognitive function.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### Further developments

* Addressing population stratification issues in the imaging genomics application example.
* Application to functional connectome data arising from different fMRI runs for the same set of subjects.
  - NB vs. EM vs. Rest.
  - Expecting results consistent with functional connectome individuality.
  - Functional network connectivity patterns can also be calculated from group spatial ICA time courses.
            </script>
          </section>
        </section>

        <section>
          <section>
            <h3>FDR-Corrected SCCA References</h3>
<ul>
  <li>Gossmann, A., Zille, P., Calhoun, V., &amp; Wang, Y.-P. (2017). FDR-Corrected Sparse Canonical Correlation Analysis with Applications to Imaging Genomics. <a href="http://arxiv.org/abs/1705.04312">arXiv:1705.04312</a> [<a href="http://arxiv.org/pdf/1705.04312">pdf</a>] <i>(under review in IEEE/TMI)</i>
  </li>

  <li>Associated code: <a href="https://github.com/agisga/FDRcorrectedSCCA">https://github.com/agisga/FDRcorrectedSCCA</a> </li>
</ul>
          </section>
        </section>

        <section data-markdown>
          <script type="text/template">
# The End

# Thank you
          </script>
        </section>


      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        history: true,
        slideNumber: true,
        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // MathJax integration
        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'
        },

        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/math/math.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });

      Reveal.configure({ pdfMaxPagesPerSlide: 1 });
    </script>
  </body>
</html>
