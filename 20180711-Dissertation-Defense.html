<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Bioinnovation PhD Program – Dissertation Defense - Alexej Gossmann</title>

    <meta name="description" content="Bioinnovation PhD Program - Dissertation Defense">
    <meta name="author" content="Alexej Gossmann">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Emoji support -->
    <!-- See: https://afeld.github.io/emoji-css/ -->
    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!-- MathJax macros (inline math delimiters, new commands, etc.), more configuration under Reveal.initialize below -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ["$","$"] ],
          displayMath: [ ["$$","$$"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: { autoNumber: "AMS" },
          Macros: {
            subscript: ['_{#1}', 1],
            superscript: ['^{#1}', 1],
            fatu: '\\mathbf{u}',
            fatv: '\\mathbf{v}',
            fatlambda: '\\boldsymbol{\\lambda}',
            R: '\\mathbb{R}'
          }
        }
      });
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h3>Regaining control of false findings in feature selection, classification, and prediction on neuroimaging and genomics data</h3>
          <p>
            <small>Oral defense of a dissertation submitted to the Bioinnovation PhD Program of the School of Science and Engineering of Tulane University in partial fulfillment of the requirements for the PhD degree by</small>
          </p>
          <h4>Alexej Gossmann</h4>
          <p>July 11, 2018</p>
          <aside class="notes">
            <p>I will present to you my dissertation work on machine learning methodology for neuroimaging genomics.</p>
            <p>I won't have the time to go much into detail, since I have many projects to discuss.
            But I will be happy to answer any questions you have after the talk.</p>
          </aside>
        </section>

        <!-- Precision Medicine -->
        <section>
          <section>
            <h2>Precision Medicine</h2>
            <p>Inter-personal diversity in the patients' biology</p>
            <p class="fragment fade-right">⇾ differences in disease susceptibility/progression</p>
            <p class="fragment fade-right">⇾ differences in treatment efficacy</p>
            <p class="fragment fade-right">⇾ "personalized" treatment plans.</p>
          </section>

          <section>
            <h2>Precision Medicine</h2>
            <ul>
              <li>New drugs and devices targeting specific subpopulations (or even individuals).</li>
              <li>No more treatment based on trial-and-error.</li>
              <li>↑ Quality of healthcare</li>
              <li>↓ Treatment time and cost.</li>
            </ul>
          </section>

          <section>
            <h2>Precision Medicine</h2>
            Made possible by:
            <ol>
              <li class="fragment highlight-green">Big data including <b>genomics</b> and <b>neuroimaging</b>.</li>
              <li class="fragment highlight-red">Computational methods including <b>machine learning</b> and <b>modern statistics</b>.</li>
            </ol>
          </section>
        </section>

        <!-- Review of genomics and neuroimaging -->
        <section>
          <section data-background="#0f829d">
            <small>From left to right: (1) gene region on a chromosome; (2) chemical structure of DNA; (3) transcription/translation of genes into ncRNA, mRNA, protein.</small>
            <img width="380" src="./img/20180711-Dissertation-Defense/Chromosome_DNA_Gene.svg.png" alt="Gene region on a chromosome">
            <img width="500" src="./img/20180711-Dissertation-Defense/DNA_chemical_structure_2.svg.png" alt="Chemical structure of genes">
            <img width="390" src="./img/20180711-Dissertation-Defense/DNA_to_protein_or_ncRNA.svg.png" alt="Transcription and translation of genes">
            <br>
            <small>Source: Images by Thomas Shafee [<a href="http://creativecommons.org/licenses/by/4.0">CC BY 4.0</a>] via Wikimedia Commons.</small>

            <aside class="notes">
              <p>First let me give you some background on where genomic and neuroimagning data comes from.
              A gene is a section of DNA.
              DNA is physically encoded in four molecules (nucleobases).
              Genes encode proteins or non-coding RNA, which have a biological function.
              Genomic data include information on the structure of the DNA, and amount of protein produced.
            </aside>
          </section>
          <section data-background="#0f829d">
            <ul>
              <li>Structural MRI: anatomical structure of the brain.</li>
              <li>Functional MRI: brain activity associated with blood flow related to energy use by brain cells.</li>
            </ul>
            <img width="270" src="./img/20180711-Dissertation-Defense/Parasagittal_MRI_of_human_head_in_patient_with_benign_familial_macrocephaly_prior_to_brain_injury_(ANIMATED).gif" alt="Structural MRI animation">
            <img width="270" src="./img/20180711-Dissertation-Defense/T1_raw.png" alt="Structural MRI from a randomly chosen PNC subject">
            <img width="270" src="./img/20180711-Dissertation-Defense/processed_emoid_fMRI.png" alt="BOLD fMRI from a randomly chosen PNC subbject at a fixed time point">
            <small>
            <ul>
              <li>Animation by Dwayne Reed at English Wikipedia [<a href="https://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>] via <a href="https://commons.wikimedia.org/wiki/File%3AParasagittal_MRI_of_human_head_in_patient_with_benign_familial_macrocephaly_prior_to_brain_injury_(ANIMATED).gif">Wikimedia Commons.</a></li>
              <li>
                A randomly chosen subject from the Philadelphia Neurodevelopmental Cohort:
                <ul>
                  <li>T1-weighted MRI before preprocessing ($192\times 256 \times 160$ voxels).</li>
                  <li>fMRI after preprocessing ($79\times 95\times 79$ voxels at $>200$ time points).</li>
                </ul>
              </li>
            </ul>
            </small>
            <aside class="notes">
              <p>These are some MRIs of the human brain.</p>
              <p>They either supply us with anatomical structure, or brain activity over time.</p>
              <p>I mostly use fMRI data which gives you brain activity measures in brain voxels, which are 3d pixels, at discrete points in time.</p>
            </aside>
          </section>
          <section>
            <h3>Precision Med. & Mental Disorders</h3>
            <img width="600" src="./img/20180711-Dissertation-Defense/endophenotype.png" alt="Relationship: Genomics - Brain MRI - Phenotype">
            <ul>
              <li>Neuroimaging as an endophenotype.<sup>[1-2]</sup></li>
              <li>Use of fMRI to monitor & guide drug treatment.<sup>[3-5]</sup></li>
            </ul>
            <br><br>
            <p><small>[1]: Hashimoto et. al., 2015, [2]: Poline et. al., 2015, [3]: Weickert et al., 2004, [4]: Apud et al., 2007, [5]: Goldstein-Piekarski et al., 2016.</small></p>
            <aside class="notes">
              <p>Mental disorders are known to be heritable, but it has proven to be very difficult to identify the causal genes of these complex diseases.
              Measures of brain function and structure, as an intermediate phenotype between genomics and clinical traits or behavioral measures, are more powerful in the identification of genomic associations, and in the interpretation of the function of the identified genes
              (because gene expression directly affects cellular metabolism, which directly influences the neural circuity).</p>
            </aside>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
            ## Reproducible Research

            Reproducibility = *re-performing the same analyses on the same data with the same code, while using a different data analyst* [Patil et al., 2016].
            <ul>
              <li>Clear documentation of methods and analyses in peer-reviewed publications.</li>
              <li>Usage of publicly accessible datasets only.</li>
              <li>Publication of free & open-source software (through [CRAN](https://cran.r-project.org/), [Bioconductor](https://www.bioconductor.org/), [PyPI](https://pypi.python.org/), [RubyGems](https://rubygems.org/), [Github](https://github.com/), etc.).</li>
              <li>One-off analysis scripts deposited on [Github](https://github.com/).</li>
              <li>Open access, [arXiv](https://arxiv.org/), [bioRxiv](https://www.biorxiv.org/), [Creative Commons](https://creativecommons.org/).</li>
            </ul>
            <aside class="notes">
            <p>Throughout my work I focus on the reproducibility of my research,
            which means that a different data analyst should be able to obtain the same data, the same code,
            and a clear description of the performed analysis steps, in order to reproduce my results.<p>
            <p>In addition, some of my methods are generally applicable, and I make them available to the broad research community
            as well-documented, free & open-source software, for example through the Comprehensive R Archive Network.</p>
            </aside>
            </script>
          </section>
        </section>

        <!-- 2 types of problems: feature selection and prediction -->
        <section>
          <section>
            <h2>Models</h2>
            <p>
            $$y = f(x\subscript{1}, x\subscript{2}, \ldots, x\subscript{p}) + \varepsilon,$$
            where $x\subscript{1}, x\subscript{2}, \ldots, x\subscript{p}$ are predictor variables, $\varepsilon$ is random noise, and $y$ is the phenotype.</p>
            <p class="fragment fade-up">Human DNA $\approx 3\cdot 10^9$ base pairs ⇝ vast majority not related to phenotype of interest
            <i class="fragment highlight-red">⇝ sparse models</i>
            $$\Rightarrow y = f(x\subscript{a\subscript{1}}, x\subscript{a\subscript{2}}, \ldots, x\subscript{a\subscript{m}}) + \varepsilon,$$
            where $\{a\subscript{1}, a\subscript{2}, \ldots, a\subscript{m}\} \subset \{1,2,\ldots,p\}$ is a small subset ($m \ll p$).</p>
            <aside class="notes">
              <p>Before I get into the details let me give some brief background on statistical methods.</p>
              <p>A statistical model is essentially a randomized function that maps a set of predictor variables to an outcome variables.</p>
            </aside>
          </section>

          <section>
            <h3>Sparse Models</h2>
            <img width = 400 src="./img/20180711-Dissertation-Defense/sparsity.png" alt="The sparsity assumption in statistical modeling">
          </section>

          <section>
            <h3>The two-faced model selection problem</h3>
            <p style="text-align:left;">
              <i class="em em-arrow_right"></i> <strong>Prediction:</strong>
              <span style="float:right;">
                <i class="em em-arrow_right"></i> <strong>Feature selection:</strong>
              </span>
            </p>
            <p style="text-align:left;">
              <i>Find best predictions for $y$.</i>
              <span style="float:right;">
                <i>Which $x\subscript{j}$ are predictive?</i>
              </span>
            </p>
            <h3>Two types of false findings</h3>
            <p style="text-align:left;">
              <i class="em em-scream"></i> <strong>False positives.</strong>
              <span style="float:right;">
                <i class="em em-scream_cat"></i> <strong>False discoveries.</strong>
              </span>
            </p>
            <p style="text-align:left;">
              Overfitting.
              <span style="float:right;">
                Curse of dimensionality.
              </span>
            </p>
          </section>

          <section>
            <h2>Aims</h2>
            <p>Establish <strong>guarantees</strong> on...</p>
            <ul>
              <li>false discoveries in feature selection,</li>
              <li>false predictions on new data (generalization)</li>
            </ul>
            <p>...for types of methods commonly used in the analysis of genomic and neuroimaging data.</p>
          </section>
        </section>

        <section>
          <section data-background="#003300">
            <h3>Resources and collaborators</h3>
            <ul>
              <li class="fragment">The Multiscale Bioimaging and Bioinformatics Laboratory (MBB) at Tulane University.</li>
              <li class="fragment">Tulane Center for Bioinformatics and Genomics (CBG).</li>
              <li class="fragment">FDA, Office of Science and Engineering, Division of Imaging, Diagnostics, and Software Reliability.</li>
              <li class="fragment">Other: The Mind Research Network, University of Wrocław, Indiana University Bloomington, University of Tennessee Health Science Center.</li>
            </ul>
            <aside class="notes">
              <p>At this point I would like to acknowledge my collaborators at Tulane at beyond...</p>
              <p>Most importantly Dr. Yu-Ping Wang's MBB lab at Tulane.</p>
            </aside>
          </section>
        </section>

        <!-- Feature selection background -->
        <section>
          <section>
            <h3>Feature selection in genomics and neuroimaging</h3>
            <ul>
              <li>Prediction of a phenotype based on few features.
                <br><span class="fragment">↳ Inexpensive diagnosis.</span>
              </li>
              <li>Elimination of noisy or redundant features.
                <br><span class="fragment">↳ More accurate prediction.</span>
              </li>
              <li>Data-generated hypotheses.
                <br><span class="fragment">↳ Biological insights.</span>
              </li>
            </ul>
            <aside class="notes">
              <p>First I will focus on the feature selection problem, and later I will talk about the prediction problem.</p>
            </aside>
          </section>
          <section>
            <h3>Multiple hypotheses testing</h3>
              <p>
              <strong>Feature selection</strong> as testing of hypotheses:
              $$H\subscript{i} : \beta\subscript{i} = 0, \quad i = 1,\ldots,p.$$
              </p>
              <ul>
                <li>$\beta\subscript{i} := $ effect of $i$th feature.</li>
                <li>$R := $ number of rejected hypotheses.</li>
                <li>$V := $ number of false rejections (i.e., Type I errors).</li>
                <li class="fragment highlight-green"><strong>Family-wise error rate:</strong> $\mathrm{FWER} = \mathbb{P}(V \geq 1)$.<sup>(*)</sup></li>
                <li class="fragment highlight-green"><strong>False discovery rate:</strong> $\mathrm{FDR} = \mathbb{E}\left( \frac{V}{\min\{R, 1\}} \right)$.<sup>(**)</sup></li>
              </ul>
              <br><br>
              <p><small>
                (*): E.g., Bonferroni, Holm (1979), Hommel (1988).
                <br>
                (**): E.g., Benjamini-Hochberg (1995), Benjamini-Yukutieli (2001).
              </small></p>
              <aside class="notes">
                <p>By design the FDR or FWER will be controlled.
                E.g, GWAS.</p>
                <p>But: Like trying to understand a sentence by analysing each letter separately, one at a time.</p>
              </aside>
          </section>

          <section>
            <h3>Sparse regression</h3>
            <p>$$\hat{\boldsymbol{\beta}} = \arg\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert\mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \lambda\lVert\mathbf{b}\rVert\subscript{1}.$$</p>
            <ul>
              <li>$\mathbf{y} = f(X) + \boldsymbol{\varepsilon} \approx f(X) = X\boldsymbol{\beta} \approx X\hat{\boldsymbol{\beta}}$.</li>
              <li>Yields a sparse solution $\hat{\boldsymbol{\beta}}$.</li>
              <li>Computationally efficient (convex).</li>
              <li>Very useful in practice.</li>
              <li class="fragment highlight-green"><strong>Problem:</strong> how sparse should $\hat{\boldsymbol{\beta}}$ be?</li>
              <li class="fragment highlight-green"><strong>Problem:</strong> how to do statistical inference on $\hat{\boldsymbol{\beta}}$?</li>
            </ul>
            <aside class="notes">
              <p>All features are analysed jointly (possible to include interaction effects, non-linear effects).</p>
              <p>Feature selection and prediction is performed simultaneously.</p>
              <p>A subset of significant features is selected from the whole set of features <i>in one step</i>, rather than "one letter at a time".</p>
              <p>Cannot control for the error rates mentioned on the last slide.</p>
            </aside>
          </section>
        </section>

        <!-- SLOPE and Group SLOPE -->
        <section>
          <section>
            <h4>Sorted L-One Penalized Estimation<sup>[1]</sup></h4>
            <p>$\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}} = \mathrm{argmin}\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert \mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \sum\subscript{i=1}^p \lambda\subscript{i} |\mathbf{b}|\subscript{(i)},$</p>
            <p>where $\lambda\subscript{1} \geq \lambda\subscript{2} \geq \ldots \geq \lambda\subscript{p} \geq 0$; and $|b|\subscript{(1)} \geq |b|\subscript{(2)} \geq \ldots \geq |b|\subscript{(p)}$ denotes the order statistic of the magnitudes of the vector $\mathbf{b}\in\mathbb{R}^p$.</p>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> Given $q\in(0,1)$, there is a procedure to choose $\boldsymbol{\lambda}$ s.t. $\mathrm{FDR}(\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}}) \leq q$ is guaranteed. <i class="fragment">...if the explanatory variables have very small pair-wise correlations.</i> <i class="fragment" style="color: #ff2c2d;"> ← typically not the case in genomics.</i></p>
            <p><small>[1]: Bogdan et. al., Annals Appl Stat, 2015.</small></p>
          </section>

          <section>
            <h2>Group SLOPE Motivation</h2>
            <ul>
              <li>Divide the data into groups by correlation. <i class="fragment" style="color: #17ff2e;"> ← Often possible for biological data.</i></li>
              <li class="fragment">Then select/drop entire groups rather than individual variables.</li>
              <li class="fragment">Redefine FDR w.r.t. groups: <strong>gFDR</strong>.</li>
            </ul>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group-wise false discovery rate

* $\mathrm{Rg} := $ "# groups discovered by Group SLOPE"
* $\mathrm{Vg} := $ "# falsely discovered groups"

We define
$$\mathrm{gFDR} := \mathrm{E} \left( \frac{\mathrm{Vg}}{\max(\mathrm{Rg}, 1)} \right).$$
            </script>
          </section>

          <section>
            <h2>Group SLOPE</h2>
            <ul>
              <li>$\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$, $X\in\mathbb{R}^{n\times p}$, $\boldsymbol{\beta}\in\mathbb{R}^p$, $\boldsymbol{\varepsilon}\sim\mathrm{N}(0, \sigma\subscript{\varepsilon}^2 I)$.</li>
              <li>$\boldsymbol{\beta}$ divided into $J$ groups of sizes $p_1, p_2, \cdots, p_J$, i.e. $\boldsymbol{\beta} = (\boldsymbol{\beta}_1^T, \boldsymbol{\beta}_2^T, \ldots, \boldsymbol{\beta}_J^T)^T$ with $\boldsymbol{\beta}_i \in \mathbb{R}^{p_i}$.</li>
            </ul>
            <p class="fragment fade-down"><i class="em em-arrow_down"></i>
            $$\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \left\lVert\mathbf{y} - X\mathbf{b}\right\rVert\subscript{2}^2 + \sum\subscript{i=1}^J \lambda\subscript{i} \sqrt{p\subscript{(i)}}\left\lVert X\subscript{(i)} \mathbf{b}\subscript{(i)}\right\rVert\subscript{2},$$
            where $\sqrt{p\subscript{(1)}}\left\lVert  X\subscript{(1)} \mathbf{b}\subscript{(1)} \right\rVert\subscript{2} \geq \sqrt{p\subscript{(2)}}\left\lVert  X\subscript{(2)} \mathbf{b}\subscript{(2)} \right\rVert\subscript{2} \geq \ldots$</p>
          </section>

          <section>
            <h3>Computational algorithms</h3>
            <p>The minimization problem can be rewritten as a sum of a convex function $f\subscript{2}$ and a differentiable convex function $f\subscript{1}$ with a Lipschitz continuous derivative:</p>
            <p>$\min\subscript{\mathbf{c}\in\mathbb{R}^p} f\subscript{1}(\mathbf{c}) + f\subscript{2}(\mathbf{c}).$</p>
            <p>Different ways to find the global solution:</p>
            <ul>
              <li><i class="fragment highlight-green">Fast iterative shrinkage-thresholding algorithm (FISTA)</i> &mdash; a proximal gradient method used in [1-3].</li>
              <li><i class="fragment highlight-green">Alternating direction method of multipliers (ADMM)</i> &mdash; derived in the thesis.</li>
            </ul>
            <p><small>
              [1]: Gossmann et. al., 2015. [2]: Brzyski, Gossmann, et. al., 2018. [3]: Gossmann et. al., 2018.
            </small></p>
          </section>

          <section>
            <img width="555" src="./img/20180711-Dissertation-Defense/FISTA_vs_ADMM_runtimes.png" alt="Comparison of FISTA and ADMM optimizers for Group SLOPE">
            <p><small>Runtime comparison of FISTA vs. ADMM optimization algorithms for Group SLOPE<br>(Figure 3.2 in the thesis).</small></p>
          </section>

          <section>
            <h3>Group SLOPE - Theoretical guarantees</h3>
            <ul>
              <li>Given a user-specified $q \in (0, 1)$,
                we show how to construct $\boldsymbol{\lambda}$ such that $\mathrm{gFDR} \leq q$.<sup>[1-3]</sup>
                <small><b>Different approaches:</b> theoretical for orthogonal designs<sup>[2-3]</sup>, heuristic based on theory for general designs<sup>[1-2]</sup>, Monte Carlo based<sup>[3].</sup></small>
                <br>
                <span class="fragment"><i class="em em-arrow_right"></i> Confirmed with extensive simulation studies on synthetic and real data.<sup>[1-3]</sup></span>
              </li>
              <li class="fragment">Asymptotically minimax estimation.<sup>[2]</sup></li>
            </ul>
            <br>
            <br>
            <p><small>
              [1]: Gossmann et. al., 2015. [2]: Brzyski, Gossmann, et. al., 2018. [3]: Gossmann et. al., 2018.
            </small></p>
          </section>

          <section data-markdown>
            <script type="text/template">
![Chromosome 22 SNPs, simulated phenotype](./img/20180711-Dissertation-Defense/gFDR_non-ortho.png)
<small> $X \in \mathbb{R}^{8915\times 5976}$ contains real DNA sequence data of chromosome 22; 726 groups (lengths from 1 to 1657, mean=$8.23$, median=1); between-group correlation < 0.3; **simulated response** $\mathbf{y} = X\boldsymbol{\beta} + \mathbf{z},$ where $\mathbf{z} \sim \mathcal{N}(0, I)$. </small>
            </script>
          </section>

          <section>
            <h3>Application - Framingham Cohort</h3>
            <ul>
              <li>SNP data for 8915 subjects.</li>
              <li>1771 subjects have corresponding spine BMD measurements.</li>
              <li>The remaining ~7000 subjects used to group SNPs.</li>
            </ul>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> $X$ with dimensions $1771 \times 117933$, consisting of 6403 groups of average size 18.42 (median size 2).</p>
          </section>
          <section data-markdown>
            <script type="text/template">
### Group SLOPE results

* 40 SNPs were selected by Group SLOPE with target gFDR $q = 0.1$, and mapped to nearby genes.
* 15 genes reported in previous studies:
  - BMD (SMOC1, RPS6KA5, FGFR2, GAA, SCN1A, RAB5A, SOX1, and A2BP1),
  - osteoarthritis (A2BP1, ADAM12, MATN1),
  - lumbar disc herniation (KIAA1217),
  - osteopetrosis (VAV3),
  - biology of osteoclasts, osteoblasts and osteogenesis (VAV3, SLC7A7, ADAM12, PPARD, FGFR2, PTPRU, SMOC1).
            </script>
          </section>
        </section>

        <!-- CCA Background -->
        <section>
          <section>
            <h3>Canonical Correlation Analysis</h3>
            <img width=960 src="./img/20180711-Dissertation-Defense/SCCA.png" alt="(Sparse) Canonical Correlation Analysis">
          </section>
          <section>
            <h3>Classical canonical correlation analysis</h3>
            <p>$$\mathrm{maximize}\subscript{u\in\mathbb{R}^p, v\in\mathbb{R}^q} \widehat{\mathrm{Cov}}(Xu, Yv) = \frac{1}{n} u^T X^T Y v,$$
            $$\mathrm{subject\,to} \quad \widehat{\mathrm{Var}}(Xu) = 1, \widehat{\mathrm{Var}}(Yv) = 1.$$
            [Hotelling, 1936]</p>
            <p class="fragment grow">The problem is degenerate if $n \leq \mathrm{max}\left( p, q \right)$.</p>
          </section>
          <section>
            <h3>Sparse CCA<sup>[1-2]</sup></h3>
            <p>$\mathrm{maximize}\subscript{\mathbf{u}\in\mathbb{R}^p, \mathbf{v}\in\mathbb{R}^q} \frac{1}{n} \mathbf{u}^T X^T Y \mathbf{v},$</p>
            <p>subject to</p>
            <p>$\lVert \mathbf{u} \rVert_2^2 \leq 1, \lVert \mathbf{v} \rVert_2^2 \leq 1, \lVert \mathbf{u} \rVert_1 \leq c_1, \lVert \mathbf{v} \rVert_1 \leq c_2$.</p>
            <ul>
              <li>Unique solution even when $p_X, p_Y \gg n$.</li>
              <li class="fragment grow">Selection of the sparsity parameters remains a challenging problem.</li>
            </ul>
            <br><br>
            <p><small>[1]: Witten et. al., 2009, [2]: Parkhomenko et. al., 2009.</small></p>
          </section>
        </section>

        <!-- Sparse CCA with FDR control -->
        <section>
          <section>
            <h3>Sparse CCA</h3>
            <p><i class="em em-arrow_right"></i> Select sparsity parameters in a data-driven fashion, such that FDR is controlled.</p>
          </section>
          <section data-markdown>
            <script type="text/template">
### Defining FDR for sparse CCA

* Consider FDR in $\mathbf{u}$ and in $\mathbf{v}$ separately.
* Consider hypotheses tests $H_i : u_i = 0$.
* The null hypothesis $H_i$ is true if the $i$th feature in $X$ is uncorrelated with all features in $Y$, i.e., if
$$(\forall j\in\left\\{ 1, 2, \dots, p_Y \right\\}) : \rho^{XY}\subscript{i, j} = 0.$$
* Let $R\subscript{\hat{\mathbf{u}}}$ be the number of the rejected $H_i$, and $V\subscript{\hat{\mathbf{u}}}$ the number of false rejections (i.e., when $\hat{u}_i \neq 0$ but $\rho^{XY}\subscript{i, j} = 0$ for all $j$).
<p><i class="em em-arrow_right"></i> Define: $\mathrm{FDR}(\hat{\mathbf{u}}) := \mathbb{E}\left( \frac{V\subscript{\hat{\mathbf{u}}}}{\max\left\\{ R\subscript{\hat{\mathbf{u}}}, 1 \right\\}} \right).$</p>
            </script>
          </section>
          <section>
            <img width="500" src="./img/20180711-Dissertation-Defense/CCA_FD_def.png" alt="Illustration of the false discovery concept for sparse CCA">
            <p><small>
              ⇾ False discovery proportions: FDP(u) = 1/3 and FDP(v) = 1/4.
              &nbsp;&nbsp;&nbsp;&nbsp;⇾ FDR = E(FDP).
            </small></p>
          </section>
        </section>

        <!-- SlopeCCA and gSlopeCCA -->
        <section>
          <section>
            <p><strong>SlopeCCA:</strong></p>
            <p><small>
              $\mathrm{minimize}\subscript{\fatu\in\R^p, \fatv\in\R^q}\, \left\{ -\fatu^T X^T Y \fatv + \sqrt{n} J\subscript{\fatlambda^u}(\fatu) + \sqrt{n} J\subscript{\fatlambda^v}(\fatv) \right\}$,
              <br>
              subject to $\lVert\fatu\rVert\subscript{2}^2 \leq 1, \lVert\fatv\rVert\subscript{2}^2 \leq 1$.
            </small></p>
            <p><strong>gSlopeCCA:</strong></p>
            <p><small>
              $\mathrm{minimize}\subscript{\fatu\in\R^p, \fatv\in\R^q} \left\{ -\fatu^T X^T Y \fatv + \sqrt{n} J\subscript{\fatlambda^u}\left( \left( \lVert \fatu\subscript{1} \rVert\subscript{2}, \dots\right)^T \right) + \sqrt{n} J\subscript{\fatlambda^v}\left(  \left( \lVert \fatv\subscript{1} \rVert\subscript{2}, \dots \right)^T \right) \right\}$,
              subject to $\lVert\fatu\rVert\subscript{2}^2 \leq 1, \lVert\fatv\rVert\subscript{2}^2 \leq 1$.
            </small></p>
            <p><small>Where $J\subscript{\fatlambda}(\fatu) = \Sigma\subscript{i=1}^p \lambda\subscript{i} |u|\subscript{(i)}$ is the Sorted L1 Norm.</small></p>
          </section>
          <section>
            <h3>SlopeCCA and gSlopeCCA - theoretical guarantees</h3>
            <p>Asymptotic FDR guarantees if $Cov(X)$ and $Cov(Y)$ are block-diagonal.</p>
            <p>($Cov(X, Y)$ can be of arbitrary shape)</p>
          </section>
          <section>
            <img width="900" src="./img/20180711-Dissertation-Defense/TCGA_gslopeCCA_canonical_variates.png" alt="Results from an application of gSlopeCCA to the Cancer Genome Atlas data">
            <p><strong>Application example:</strong> Results of gSlopeCCA applied to methylation and mRNA data from 12 diseases available from the Cancer Genome Atlas data.</p>
            <aside class="notes">
              <p>Apart from looking at the results of feature selection (i.e., which genes got selected),
              we can also look at the canonical variates.
              These derived features can reveal new information about disease phenotypes,
              and can be used for dimensionality reduction, classification, and prediction.
              Here we see that by looking at a pair of canonical variates (only two features!)
              one can distinguish different types of cancer cells rather well.</p>
            </aside>
          </section>
        </section>

        <!-- FDR-corrected SCCA -->
        <section>
          <section>
            <h2>FDR-corrected sparse CCA</h2>
            <p>A split-sample, two-step procedure:</p>
            <ol>
              <li>Split the data in two parts.</li>
              <li><strong>Using the first subsample:</strong> obtain initial estimates $\hat{\fatu}^{(0)}$ and $\hat{\fatv}^{(0)}$ using conventional sparse CCA.</li>
              <li><strong>Using the second subsample:</strong> test hypotheses of the form,
                $$\mathrm{H}^{(u)}\subscript{i} : u\subscript{i}^{(0)} = 0, \quad \mathrm{H}^{(v)}\subscript{j} : v\subscript{j}^{(0)} = 0,$$
                and adjust for multiple comparisons to control the FDR.
              </li>
            </ol>
            <aside class="notes">
              I modified my approach.
            </aside>
          </section>
          <section>
            <h3>FDR-corrected sparse CCA - Theoretical Guarantees</h3>
            <ol>
              <li>Equivalence to hypotheses:
  $$\mathrm{H}^{(u)}\subscript{i} : \left(X^T Y \hat{\fatv}^{(0)}\right)\subscript{i} = 0, \quad \mathrm{H}^{(v)}\subscript{j} : \left(Y^T X \hat{\fatu}^{(0)}\right)\subscript{j} = 0,$$
              </li>
              <li>After approximating the distribution of $X^T Y \hat{\fatv}^{(0)}$, we can use Benjamini-Hochberg to control FDR.</li>
            </ol>
            <p class="fragment"><i class="em em-arrow_right"></i> Confirmed with extensive simulation studies on synthetic and real data.<sup>[1]</sup>
            <br>
            <br>
            <small>[1]: Gossmann et. al., IEEE TMI, 2018.</small></p>
          </section>
          <!--<section>
            <h3>Simulation Studies</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/high_dim_FDR_combined.png" alt="Simulation studies of the FDR-corrected sparse CCA method.">
            <p>$X, Y \in \R^{600 \times 1500}$.
            FDR is controlled regardless of the sparsity of the true solution.</p>
          </section>
          <section>
            <h3>Simulation Studies</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/high_dim_TPR_combined.png" alt="Simulation studies of the FDR-corrected sparse CCA method.">
            <p>True positive rate is on par with competing methods.</p>
          </section>-->
          <section>
            <h3>FDR-corrected SCCA - Application</h3>
            <ul>
              <li>Diversity in brain activity and brain connectivity in children and adolescents.</li>
              <li>What are the driver genes?</li>
              <li>Relationship to neurodevelopmental and psychiatric disorders.</li>
            </ul>
          </section>
          <section>
            <h3>Dataset</h3>
            <blockquote>
              The Philadelphia Neurodevelopmental Cohort (PNC) is a large-scale collaborative study between the Brain Behaviour Laboratory at the University of Pennsylvania and the Children's Hospital of Philadelphia. It contains a fractal $n$-back fMRI task, an emotion identification fMRI task, SNP arrays, and questionnaire data for over 900 adolescents.
            </blockquote>
            <aside class="notes">
              <p>Including subjects with increased (prodromal) symptoms of ADD (107 PNC subjects), schizophrenia (103 PNC subjects), and depression (85 PNC subjects) [Kaufmann et. al., 2017].</p>
            </aside>
          </section>
          <section>
            <h3>Objective</h3>
            <blockquote>
            Use sparse CCA to identify the relationships between brain activity, brain connectivity, and genomics.
            </blockquote>
          </section>
          <section>
            <h3>n-back fMRI vs. SNPs</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/voxels_vs_genes.png" alt="PNC results: n-back task fMRI vs. SNP analysis">
            <p><small>Selection from $85796$ brain voxels and $60372$ genomic features.</small></p>
          </section>
          <section>
            <h4>Results validation - n-back fMRI vs. SNPs</h4>
            <img width="260" src="./img/20180711-Dissertation-Defense/voxels_vs_genes.png" alt="PNC results: n-back task fMRI vs. SNP analysis">
            <ol>
              <li>Similar brain regions have been found in other fMRI studies of working memory.</li>
              <li>At least 34 out of the 65 identified genes have been previously associated with various aspects of human cognitive function.</li>
            </ol>
          </section>
          <section>
            <h3>Functional connectivity (FC) vs. SNPs</h3>
            <ol>
              <li>Emotion identification task fMRI data transformed to FC measures.</li>
              <li>FDR-corrected sparse CCA solution includes 129 genomic features and 107 FC features.</li>
            </ol>
          </section>
          <section>
            <h4>FC vs. SNPs - Top 10 selected genes</h4>
            <small>
            <table>
              <thead>
                <tr>
                  <th>Gene</th>
                  <th>Previously studies in association with...</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>DAB1   </td>
                  <td>Autism, schizophrenia, brain development</td>
                </tr>
                <tr>
                  <td>NAV2   </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>WWOX   </td>
                  <td>Cognitive ability, brain development</td>
                </tr>
                <tr>
                  <td>CNTNAP2</td>
                  <td>Autism, brain connectivity, brain development, schizophrenia, major depression, cognitive ability (linguistic processing)</td>
                </tr>
                <tr>
                  <td>NELL1  </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>PTPRT  </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>FHIT   </td>
                  <td>Cognitive ability, autism, ADHD</td>
                </tr>
                <tr>
                  <td>MACROD2</td>
                  <td>Autism</td>
                </tr>
                <tr>
                  <td>LRP1B  </td>
                  <td>Cognitive function</td>
                </tr>
                <tr>
                  <td>DGKB   </td>
                  <td>Brain development, bipolar disorder</td>
                </tr>
              </tbody>
            </table>
            </small>
            <p><small>(for detail see [Gossmann et. al., TMI, 2018])</small></p>
          </section>
        </section>

        <!-- Thresholdout -->
        <section>
          <section>
            <h3>Another type of false findings</h3>
            <p><i class="em em-white_check_mark"></i> Feature selection with FDR control.</p>
            <p><i class="em em-arrow_right"></i> Features can be used to fit a predictive model.</p>
            <p class="fragment grow">Danger of over-fitting to the local noise in the given dataset, resulting in false predictions on new data.</p>
            <p>What to do?</p>
          </section>
          <section>
            <p>In Machine Learning practice, generally, usage of two independent datasets &mdash; <i>"training"</i> and <i>"test"</i> data.</p>
            <p class="fragment"><strong>Training data:</strong> exploratory analysis, model fitting, parameter tuning, comparison of different machine learning algorithms, feature selection, etc.</p>
            <p class="fragment fade-right">$\Longrightarrow$ Adaptive machine learning, risk of overfitting.</p>
            <p class="fragment"><strong>Test data:</strong> Performance evaluation <i>after the trained machine learning algorithm has been "frozen"</i>.</p>
            <p class="fragment fade-right">$\Longrightarrow$ Accurate performance measures of the final model, <strong>if the test data is used only once</strong>.</p>
              <aside class="notes">
              The performance metric obtained on test data will make it evident whether any overfitting has occurred during model training. Other techniques (multiple testing, cross-validation, bootstrap) can be used to avoid or reduce the overfitting on the training data.
              </aside>
          </section>
          <section data-transition="slide-in fade-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_1.png" alt="Test data reuse 1">
          </section>
          <section data-transition="fade-in fade-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_2.png" alt="Test data reuse 2">
          </section>
          <section data-transition="fade-in slide-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_3.png" alt="Test data reuse 3">
          </section>
          <section>
            <h2>Idea</h2>
            <p>Can we <strong>obfuscate</strong> the test data to avoid overfitting?</p>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> Differential privacy.<sup>[1]</sup></p>
            <p class="fragment">Promising simulation results.<sup>[2-3]</sup>
            <br><br><br>
            <small>[1]: Dwork, McSherry, Nissim, Smith, 2006.<br> [2]: Dwork et. al., Science, 2015.<br> [3]: Gossmann et. al., SPIE 2018.</small></p>
          </section>
          <section>
            <h2>Next</h2>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> Investigation of the overfitting behavior resulting from the use of modern "black box" machine learning algorithms.</p>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> Apply our test data reuse method to real neuroimaging and genomic data.</p>
          </section>
        </section>

        <section>
          <section data-background="#093748">
            <p><strong>Parts of this work appear in:</strong></p>
            <ol>
              <li><strong>G.A.</strong>, Cao, S., &amp; Wang, Y.-P. In proceedings of ACM BCB '15. 2015.</li>
              <li><strong>G.A.</strong>, Cao, S., Brzyski, D., Zhao, L. J., Deng, H. W., &amp; Wang, Y. P. IEEE/ACM TCBB. 2017.</li>
              <li>Brzyski, D., <strong>G.A.</strong>, Su, W., &amp; Bogdan, M. JASA. 2018.</li>
              <li><strong>G.A.</strong>, Zille, P., Calhoun, V., &amp; Wang, Y.-P. IEEE TMI. 2018.</li>
              <li><strong>G.A.</strong>, Pezeshk, A., &amp; Sahiner, B. In proceedings of SPIE Medical Imaging '18. 2018.</li>
            </ol>
          </section>
        </section>

        <!-- Last slide -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/dna-with-equations.jpg">
            <img width="480" src="./img/20180711-Dissertation-Defense/Parasagittal_MRI_of_human_head_in_patient_with_benign_familial_macrocephaly_prior_to_brain_injury_(ANIMATED).gif" alt="Structural MRI animation">
          </section>
        </section>

      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>

      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // MathJax integration
        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'
        },

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/search/search.js', async: true },
          // Zoom in and out with Alt+click
          { src: 'plugin/zoom-js/zoom.js', async: true },
          // Speaker notes
          { src: 'plugin/notes/notes.js', async: true },
          // MathJax
          { src: 'plugin/math/math.js', async: true }
        ]
      });

    </script>

  </body>
</html>
