<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Bioinnovation PhD Program – Dissertation Defense - Alexej Gossmann</title>

    <meta name="description" content="Bioinnovation PhD Program - Dissertation Defense">
    <meta name="author" content="Alexej Gossmann">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Emoji support -->
    <!-- See: https://afeld.github.io/emoji-css/ -->
    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!-- MathJax macros (inline math delimiters, new commands, etc.), more configuration under Reveal.initialize below -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ["$","$"] ],
          displayMath: [ ["$$","$$"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: { autoNumber: "AMS" },
          Macros: {
            subscript: ['_{#1}', 1],
            superscript: ['^{#1}', 1],
            fatu: '\\mathbf{u}',
            fatv: '\\mathbf{v}',
            fatlambda: '\\boldsymbol{\\lambda}',
            R: '\\mathbb{R}'
          }
        }
      });
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h3>Regaining control of false findings in feature selection, classification, and prediction on neuroimaging and genomics data</h3>
          <p>
          <small>Oral defense of a dissertation submitted to the <b>Bioinnovation PhD Program</b> of the <b>School of Science and Engineering</b> of <b>Tulane University</b> in partial fulfillment of the requirements for the PhD degree by</small>
          </p>
          <h4>Alexej Gossmann</h4>
          <p>July 11, 2018</p>
          <aside class="notes">
            <p>I will present to you my dissertation work on machine learning methodology for neuroimaging genomics.</p>
            <p>In the interest of time, I'll skip some of the details, so just let me know if you have questions.</p>
          </aside>
        </section>

        <!-- Precision Medicine -->
        <section>
          <section>
            <h2>Precision Medicine</h2>
            <p>Inter-personal diversity in the patients' biology</p>
            <p class="fragment fade-right">⇾ "personalized" treatment plans</p>
            <p class="fragment fade-up">↑ Quality of healthcare</p>
            <p class="fragment fade-down">↓ Treatment time and cost</p>
            <aside class="notes">
              <p>The motivation and the ultimate purpose of this research is precision medicine.</p>
              <p>Differences in disease susceptibility/progression between subpopulations/individual</p>
              <p>...lead to differences in treatment efficacy</p>
              <p>...but also enable "personalized" treatment plans.</p>
              <p>New drugs and devices targeting specific subpopulations (or even individuals).</p>
              <p>No more treatment based on trial-and-error.</p>
            </aside>
          </section>

          <section>
            <h2>Precision Medicine</h2>
            Made possible by:
            <ol>
              <li class="fragment highlight-green">Big data including <b>genomics</b> and <b>neuroimaging</b>.</li>
              <li class="fragment highlight-red">Computational methods including <b>machine learning</b> and <b>modern statistics</b>.</li>
            </ol>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>
        </section>

        <!-- Review of genomics and neuroimaging -->
        <section>
          <section data-background="#0f829d">
            <small>From left to right: (1) gene region on a chromosome; (2) chemical structure of DNA; (3) transcription/translation of genes into ncRNA, mRNA, protein.</small>
            <img width="380" src="./img/20180711-Dissertation-Defense/Chromosome_DNA_Gene.svg.png" alt="Gene region on a chromosome">
            <img width="500" src="./img/20180711-Dissertation-Defense/DNA_chemical_structure_2.svg.png" alt="Chemical structure of genes">
            <img width="390" src="./img/20180711-Dissertation-Defense/DNA_to_protein_or_ncRNA.svg.png" alt="Transcription and translation of genes">
            <br>
            <small>Source: Images by Thomas Shafee [<a href="http://creativecommons.org/licenses/by/4.0">CC BY 4.0</a>] via Wikimedia Commons.</small>

            <aside class="notes">
              <p>First let me give you some background on where genomic and neuroimagning data comes from.
              A gene is a section of DNA.
              DNA is physically encoded in four molecules (nucleobases).
              Broadly speaking, genes encode proteins or non-coding RNA, which have a biological function.
              Genomic data include information on the structure of the DNA, and amount of protein produced, which can all be expressed/stored in numeric form.
            </aside>
          </section>

          <section data-background="#0f829d">
            <ul>
              <li>Structural MRI: anatomical structure of the brain.</li>
              <li>Functional MRI: brain activity associated with blood flow related to energy use by brain cells across time.</li>
            </ul>
            <div style="font-size: 0.6em;">
            <img width="270" src="./img/20180711-Dissertation-Defense/T1.gif" alt="Structural MRI animation">
            <img width="270" src="./img/20180711-Dissertation-Defense/T2star_raw.gif" alt="Raw BOLD fMRI (n-back task) from a randomly chosen PNC subject at a fixed time point">
            <img width="270" src="./img/20180711-Dissertation-Defense/T2star_preproc.gif" alt="SPM12 pre-processed BOLD fMRI (n-back task) from a randomly chosen PNC subbject at a fixed time point">
            <br>
            A randomly chosen subject from the Philadelphia Neurodevelopmental Cohort:
            <ul>
              <li>T1-weighted MRI before preprocessing ($192\times 256 \times 160$ voxels).</li>
              <li>n-back task BOLD fMRI (i.e., T2*-weighted) before preprocessing ($64\times 64\times 49$ voxels), and after preprocessing ($79\times 95\times 79$ voxels).</li>
            </ul>
            </div>
            <aside class="notes">
              <p>These are some MRIs of the human brain.</p>
              <p>They either supply us with anatomical structure, or brain activity over time.</p>
              <p>MRI 3D volumes are collected as 2D axial slices, as I have visualized in the shown animations for a randomly chosen PNC subject (a 9 years old female).</p>
              <p>You can see a raw structural or T1-weighted MRI on the left, a raw BOLD fMRI in the middle, and an fMRI after preprocessing on the right.</p>
              <p>I mostly use BOLD fMRI data which gives you brain activity measures in brain voxels, which are 3d pixels, at discrete points in time.</p>
            </aside>
          </section>

          <section data-background="./img/20180711-Dissertation-Defense/T2star_preproc_facetted.gif" data-background-size="900px" data-background-color="#0f829d">
            <p>[230 time points]</p>
            <aside class="notes">
              <p>Here you see the data for the same random subject visualized at different time points.</p>
            </aside>
          </section>

          <section>
            <h3>Precision Med. & Mental Disorders</h3>
            <img width="600" src="./img/20180711-Dissertation-Defense/endophenotype.png" alt="Relationship: Genomics - Brain MRI - Phenotype">
            <ul>
              <li>Neuroimaging as an endophenotype.<sup>[1-2]</sup></li>
              <li>Use of fMRI to aid diagnosis, or to monitor &amp; guide drug treatment.<sup>[3-5]</sup></li>
            </ul>
            <br><br>
            <p><small>[1]: Hashimoto et. al., 2015, [2]: Poline et. al., 2015, [3]: Weickert et al., 2004, [4]: Apud et al., 2007, [5]: Goldstein-Piekarski et al., 2016.</small></p>
            <aside class="notes">
              <p>To bring this back to the topic of precision medicine:</p>
              <p>Mental disorders are known to be heritable, but it has proven to be very difficult to identify the causal genes of these complex diseases.</p>
              <p>Measures of brain function and structure, as an intermediate phenotype between genomics and clinical traits or behavioral measures.</p>
              <p>More powerful in the identification of genomic associations.</p>
              <p>Helpful in the interpretation of the function of the identified genes, because gene expression directly affects cellular metabolism, which directly influences the neural circuity.</p>
            </aside>
          </section>
        </section>

        <!-- 2 types of problems: feature selection and prediction -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/catwalk.gif" data-background-repeat="repeat" data-background-size="400px">
            <div style="background-color: rgba(0, 0, 0, 0.8); color: #fff; padding: 40px;">
              <h1>Models</h1>
            </div>
            <aside class = "notes">
              <p>Before I present my methods and results, let me give you some brief background on statistical models.</p>
            </aside>
          </section>

          <section>
            <h2>Models</h2>
            <p>
            $$y = f\subscript{\boldsymbol{\theta}}(x\subscript{1}, x\subscript{2}, \ldots, x\subscript{p}) + \varepsilon,$$
            where $x\subscript{1}, x\subscript{2}, \ldots, x\subscript{p}$ are predictor variables, $\varepsilon$ is random noise, $y$ is the phenotype, and $\boldsymbol{\theta}$ is a vector of parameters to be estimated.</p>
            <p class="fragment fade-up">Human DNA $\approx 3\cdot 10^9$ base pairs ⇝ vast majority not related to phenotype of interest
            <i class="fragment highlight-red">⇝ sparse models</i>
            $$\Rightarrow y = f\subscript{\tilde{\boldsymbol{\theta}}}(x\subscript{a\subscript{1}}, x\subscript{a\subscript{2}}, \ldots, x\subscript{a\subscript{m}}) + \varepsilon,$$
            where $\{a\subscript{1}, a\subscript{2}, \ldots, a\subscript{m}\} \subset \{1,2,\ldots,p\}$ is a small subset ($m \ll p$).</p>
            <aside class="notes">
              <p>A statistical model is essentially a randomized function that maps a set of predictor variables to an outcome variables.</p>
              <p>More precisely, you may call the shown model a regression model.</p>
              <p>However, consider for example your favorite phenotype (like certain disease or trait) and the human DNA.</p>
              <p>The human DNA consists of 3 billion base pairs, and one cannot expect that all bases have an effect on the specific phenotype.</p>
              <p>Only the changes/mutations within a relatively small number of genes will have an effect.</p>
              <p>This brings us to the topic of sparse models.</p>
            </aside>
          </section>

          <section>
            <h3>Sparse Models</h3>
            <img width = 400 src="./img/20180711-Dissertation-Defense/sparsity.png" alt="The sparsity assumption in statistical modeling">
            <aside class="notes">
              <p>Here is an illustration of what it means to have a sparse regression model, where the distribution of the phenotype or outcome variable y conditional on the predictor variables x5, x8, and x13 is the same as the distribution conditional on all potential predictor variables x1, x2, and so on.</p>
            </aside>
          </section>

          <section data-background="#fff">
            <h3>The two-faced model selection problem</h3>
            <p style="text-align:left;">
              <i class="em em-arrow_right"></i> <strong>Prediction:</strong>
              <span style="float:right;">
                <i class="em em-arrow_right"></i> <strong>Feature selection:</strong>
              </span>
            </p>
            <p style="text-align:left;">
              <i>Find best predictions for $y$.</i>
              <span style="float:right;">
                <i>Which $x\subscript{j}$ are predictive?</i>
              </span>
            </p>
            <h3>Two types of false findings</h3>
            <p style="text-align:left;">
              <i class="em em-scream"></i> <strong>False positives.</strong>
              <span style="float:right;">
                <i class="em em-scream_cat"></i> <strong>False discoveries.</strong>
              </span>
            </p>
            <p style="text-align:left;">
              Overfitting.
              <span style="float:right;">
                Curse of dimensionality.
              </span>
            </p>
            <aside class="notes">
              <p>When choosing the best model for a particular dataset, then, this involves solving two related but fundamentally different problems at the same time.</p>
              <p>Obtain the best possible predictions of the outcome variables y based on the given predictor variables x1, x2, and so on;</p>
              <p>Find the subset of predictor variables that are associated with the outcome variable y, and dispose of the irrelevant variables.</p>
              <p>Both problems are essential in biomedical applications. The first for example for disease diagnosis or risk prediction, and the second to help understand the cause of disease, e.g., for the identification of targets for treatment.</p>
              <p>Depending on which of the two modeling problems you consider, you are dealing with a conceptionally different type of false findings.</p>
              <p>In prediction you may have FP (or FN), e.g., a misdiagnosis; while in feature selection we have false discoveries, e.g., a gene identified as associated to disease when it is not.</p>
              <p>Unfortunately, many ML methods, and also many statistical models, which are commonly used in genomics and neuroimaging, provide no way to obtain an idea of how many false findings to expect.</p>
            </aside>
          </section>

          <section data-background="#fff">
            <h2>Aims</h2>
            <p>Establish <strong>guarantees</strong> on...</p>
            <ul>
              <li class="fragment grow">false discoveries in feature selection,</li>
              <li class="fragment grow">false predictions on new data (generalization)</li>
            </ul>
            <p>...for types of methods commonly used in the analysis of genomic and neuroimaging data.</p>
            <aside class="notes">
              <p>In my thesis, I proposed new methods that build on ones widely used in genomics and neuroimaging.</p>
              <p>However, my methods focus on explicitly establishing guarantees...</p>
            </aside>
          </section>
        </section>

        <section>
          <section data-background="#003300">
            <h3>Resources and collaborators</h3>
            <ul>
              <li>The Multiscale Bioimaging and Bioinformatics Laboratory (MBB) at Tulane University.</li>
              <li>Tulane Center for Bioinformatics and Genomics (CBG).</li>
              <li>FDA, Office of Science and Engineering, Division of Imaging, Diagnostics, and Software Reliability.</li>
              <li>Other: The Mind Research Network, University of Wrocław, Indiana University Bloomington, University of Tennessee Health Science Center.</li>
            </ul>
            <aside class="notes">
              <p>At this point I would like to acknowledge my collaborators at Tulane at beyond...</p>
              <p>Most importantly Dr. Yu-Ping Wang's MBB lab at Tulane.</p>
              <p>And a few other places where people who have helped me with my research work.</p>
            </aside>
          </section>
        </section>

        <!-- Feature selection background -->
        <!-- Feature selection in regression problems -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/dna-with-equations.jpg">
            <div style="background-color: rgba(0, 0, 0, 0.8); color: #fff; padding: 20px;">
              <h3>Feature selection in genomics and neuroimaging</h3>
            </div>
            <aside class="notes">
              <p>First I will focus on the feature selection problem, and later I will talk about the prediction problem.</p>
              <p>Feature selection is important for genomics and neuroimaging data analysis for a number of reasons:</p>
              <ul>
                <li>Accuracy of prediction (removing redundant or "noisy" features),</li>
                <li>Inexpensive diagnosis (accurate prediction based on a small number of features),</li>
                <li>Data-generated biological hypotheses.</li>
              </ul>
            </aside>
          </section>

          <section>
            <h3>Multiple hypotheses testing</h3>
              <p>
              <strong>Feature selection</strong> as testing of hypotheses:
              $$H\subscript{i} : \beta\subscript{i} = 0, \quad i = 1,\ldots,p.$$
              </p>
              <ul>
                <li>$\beta\subscript{i} := $ effect of $i$th feature.</li>
                <li>$R := $ number of rejected hypotheses.</li>
                <li>$V := $ number of false rejections (i.e., Type I errors).</li>
                <li class="fragment highlight-green"><strong>Family-wise error rate:</strong> $\mathrm{FWER} = \mathbb{P}(V \geq 1)$.<sup>[1]</sup></li>
                <li class="fragment highlight-red"><strong>False discovery rate:</strong> $\mathrm{FDR} = \mathbb{E}\left( \frac{V}{\min\{R, 1\}} \right)$.<sup>[2]</sup></li>
              </ul>
              <br><br>
              <p><small>
                [1]: E.g., Bonferroni, Holm (1979), Hommel (1988).
                <br>
                [2]: E.g., Benjamini-Hochberg (1995), Benjamini-Yukutieli (2001).
              </small></p>
              <aside class="notes">
                <p>The classical way to avoid false discoveries is found in the theory on multiple hypotheses testing.</p>
                <p>There are two imporant measures of error rates in feature selection - FWER and FDR.</p>
                <p> I focus on FDR - more advantageous in a very high-dimensional setting. </p>
                <p>Existing procedures control the FDR or FWER. E.g, widely-used in GWAS.</p>
                <p>But: Doing a hypothesis test on every possible genetic variant separately is like trying to understand a sentence by analysing each letter separately, one at a time.</p>
              </aside>
          </section>

          <section>
            <h3>Sparse regression</h3>
            <p><strong>LASSO:</strong> $\hat{\boldsymbol{\beta}} = \arg\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert\mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \lambda\lVert\mathbf{b}\rVert\subscript{1}.$<small><sup>[1]</sup></small></p>
            <ul>
              <li>$\mathbf{y} = f(X) + \boldsymbol{\varepsilon} \approx f(X) = X\boldsymbol{\beta} \approx X\hat{\boldsymbol{\beta}}$.</li>
              <li>Yields a sparse solution $\hat{\boldsymbol{\beta}}$.</li>
              <li>Computationally efficient (convex).</li>
              <li>Very useful in practice.</li>
              <li class="fragment highlight grow"><strong>Problem:</strong> how to do statistical inference on $\hat{\boldsymbol{\beta}}$?</li>
              <li class="fragment highlight grow"><strong>Problem:</strong> how sparse should $\hat{\boldsymbol{\beta}}$ be?</li>
            </ul>
            <br>
            <p><small>
              [1]: Tibshirani, JRSSB, 1996.
            </small></p>
            <aside class="notes">
              <p>All features are analysed jointly (possible to include interaction effects, non-linear effects).</p>
              <p>It essentially amounts to the estimation of a coefficient vector beta, where each element may quantify...</p>
              <p>Feature selection and prediction is performed simultaneously.</p>
              <p>A subset of significant features is selected from the whole set of features <i>in one step</i>, rather than "one letter at a time".</p>
              <p><strong>Cannot control for the error rates mentioned on the last slide.</strong></p>
            </aside>
          </section>
        </section>

        <!-- SLOPE and Group SLOPE -->
        <section>
          <section>
            <h4>Sorted L-One Penalized Estimation<sup>[1]</sup></h4>
            <p>$\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}} = \mathrm{argmin}\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert \mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \sum\subscript{i=1}^p \lambda\subscript{i} |\mathbf{b}|\subscript{(i)},$</p>
            <p>where $\lambda\subscript{1} \geq \lambda\subscript{2} \geq \ldots \geq \lambda\subscript{p} \geq 0$; and $|b|\subscript{(1)} \geq |b|\subscript{(2)} \geq \ldots \geq |b|\subscript{(p)}$ denotes the order statistic of the magnitudes of the vector $\mathbf{b}\in\mathbb{R}^p$.</p>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> Given $q\in(0,1)$, there is a procedure to choose $\boldsymbol{\lambda}$ s.t. $\mathrm{FDR}(\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}}) \leq q$ is guaranteed. <i class="fragment">...if the explanatory variables have very small pair-wise correlations.</i> <i class="fragment" style="color: #ff2c2d;"> ← typically not the case in genomics.<sup>[2]</sup></i></p>
            <p><small>[1]: Bogdan et. al., Annals Appl Stat, 2015. [2]: Gossmann et. al., ACM BCB, 2015.</small></p>
            <aside class="notes">
              <p> To my knowledge, the first sparse regression method that can control the FDR is SLOPE. </p>
              <p> It provides an explicit way of setting lambda so that the FDR will be under some q. </p>
              <p> BUT this requires... </p>
              <p> Usually this isn't the case with biological data. </p>
            </aside>
          </section>

          <section>
            <h2>Group SLOPE Motivation</h2>
            <ul>
              <li>Divide the data into groups by correlation. <i class="fragment" style="color: #17ff2e;"> ← Often possible for biological data.</i></li>
              <li class="fragment">Then select/drop entire groups rather than individual variables.</li>
              <li class="fragment">Redefine FDR w.r.t. groups: <strong>gFDR</strong>.</li>
            </ul>
            <aside class="notes">
              <p> This motivated us to develop the Group SLOPE method, in which we divide the data intro groups according to the correllation between variables. </p>
              <p> This is often possible for biological data, and in particular DNA sequence data. </p>
              <p> Then we select or drop entire groups of variables rather than individual variables, ...</p>
              <p> ...and we redefine the FDR with respect to groups. </p>
            </aside>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group-wise false discovery rate

* $\mathrm{Rg} := $ "total # discovered groups"
* $\mathrm{Vg} := $ "# falsely discovered groups"

We define
$$\mathrm{gFDR} := \mathrm{E} \left( \frac{\mathrm{Vg}}{\max(\mathrm{Rg}, 1)} \right).$$
<aside class="notes">
<p> We define the groupwise FDR as the expected value of the number of falsely discovered groups over the total number of discovered groups. </p>
</aside>
<aside class="notes">
<p>...</p>
</aside>
            </script>
          </section>

          <section>
            <h2>Group SLOPE</h2>
            <ul>
              <li>$\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$, $X\in\mathbb{R}^{n\times p}$, $\boldsymbol{\beta}\in\mathbb{R}^p$, $\boldsymbol{\varepsilon}\sim\mathrm{N}(0, \sigma\subscript{\varepsilon}^2 I)$.</li>
              <li>$\boldsymbol{\beta}$ divided into $J$ groups of sizes $p_1, p_2, \cdots, p_J$, i.e. $\boldsymbol{\beta} = (\boldsymbol{\beta}_1^T, \boldsymbol{\beta}_2^T, \ldots, \boldsymbol{\beta}_J^T)^T$ with $\boldsymbol{\beta}_i \in \mathbb{R}^{p_i}$.</li>
            </ul>
            <p class="fragment fade-down"><i class="em em-arrow_down"></i>
            $$\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \left\lVert\mathbf{y} - X\mathbf{b}\right\rVert\subscript{2}^2 + \sum\subscript{i=1}^J \lambda\subscript{i} \sqrt{p\subscript{(i)}}\left\lVert X\subscript{(i)} \mathbf{b}\subscript{(i)}\right\rVert\subscript{2},$$
            where $\sqrt{p\subscript{(1)}}\left\lVert  X\subscript{(1)} \mathbf{b}\subscript{(1)} \right\rVert\subscript{2} \geq \sqrt{p\subscript{(2)}}\left\lVert  X\subscript{(2)} \mathbf{b}\subscript{(2)} \right\rVert\subscript{2} \geq \ldots$</p>
            <aside class="notes">
              <p> The Groups SLOPE method is formulated in this way, where the coefficient vector beta is divided into J groups of length p1,p2,...and so on variables. </p>
              <p> The parametric formulation of the model is very similar to SLOPE, but with the penalty term operating at a group level rather than on individual variables. </p>
            </aside>
          </section>

          <section>
            <strong>Different ways to find the global solution:</strong>
            <img width="455" style="float: right;" src="./img/20180711-Dissertation-Defense/FISTA_vs_ADMM_runtimes.png" alt="Comparison of FISTA and ADMM optimizers for Group SLOPE">
            <ul style="text-align: left; display: block; margin: 0 0 0 1em;">
              <li><i>Fast iterative shrinkage-thresholding algorithm (FISTA)</i> &mdash; a proximal gradient method used in [1-3].</li>
              <li><i>Alternating direction method of multipliers (ADMM)</i> &mdash; derived in the thesis.</li>
            </ul>
            <p style="text-align: right;"><small>[Figure 3.2 in the thesis]</small></p>
            <p><small>
              [1]: Gossmann et. al., 2015. [2]: Brzyski, Gossmann, et. al., 2018. [3]: Gossmann et. al., 2017.
            </small></p>
            <aside class="notes">
              <p>The method would be useless without a way to obtain the solution efficiently.</p>
              <p>In the thesis I derive two types of optimization routines...</p>
            </aside>
          </section>

          <section>
            <h3>Group SLOPE - Theoretical guarantees</h3>
            <p>Given a user-specified $q \in (0, 1)$, we show how to choose $\boldsymbol{\lambda}$ such that $\mathrm{gFDR} \leq q$.<sup>[1-3]</sup></p>
            <p><small><b>Different approaches:</b> theoretical for orthogonal designs<sup>[2-3]</sup>, heuristic based on theory for general designs<sup>[1-2]</sup>, Monte Carlo based for general designs<sup>[3].</sup></small></p>
            <p><span class="fragment"><i class="em em-arrow_right"></i> Confirmed with extensive simulation studies on synthetic and real data.<sup>[1-3]</sup></span></p>
            <br>
            <br>
            <p><small>
              [1]: Gossmann et. al., 2015. [2]: Brzyski, Gossmann, et. al., 2018. [3]: Gossmann et. al., 2017.
            </small></p>
            <aside class="notes">
              <p> The Group SLOPE method also provides theoretical guarantees on FDR control, which was our main goal. </p>
              <p> Given a user-specified target FDR threshold q, we give explicit procedures for selecting lambda so that the FDR will be below q. </p>
              <p> We have three types of approaches to set lambda - a fully theoretically derived lambda sequence for the special case when the groups are orthogonal to each other - a heuristic based on rigorous theory leading to FDR control under general designs for nearly uncorrellated groups - and Monte Carlo based approaches for general designs. </p>
              <p> We have confirmed with extensive simulations studies that all of these are successful in controlling the gFDR. </p>
            </aside>
          </section>

          <section data-markdown>
            <script type="text/template">
![Chromosome 22 SNPs, simulated phenotype](./img/20180711-Dissertation-Defense/gFDR_non-ortho.png)
<small> $X \in \mathbb{R}^{8915\times 5976}$ contains real DNA sequence data of chromosome 22; 726 groups (lengths from 1 to 1657, mean=$8.23$, median=1); between-group correlation < 0.3; **simulated response** $\mathbf{y} = X\boldsymbol{\beta} + \mathbf{z},$ where $\mathbf{z} \sim \mathcal{N}(0, I)$. [Figure 3.5 in the thesis] </small>
<aside class="notes">
<p> We compared the gFDR of the group SLOPE method on real DNA sequence data to the gFDR of the most commonly used method group LASSO. </p>
<p> Here, the x-axis shows the number of truly relevant groups, and the y-axis the gFDR of each method. Regardless of the number of relevant groups, which isnt known a priori, group SLOPE always keeps the FDR near the target level of 10% while group LASSO does not. </p>
<p> As you see on the right, even while keeping the gFDR low, group SLOPE keeps the true discovery proportion relatively high. </p>
</aside>
<aside class="notes">
<p>...</p>
</aside>
            </script>
          </section>

          <section>
            <h3>Application - Framingham Cohort Analysis<sup>[1]</sup></h3>
            <ul>
              <li>SNP data for 8915 subjects.</li>
              <li>1771 subjects have corresponding spine BMD measurements.</li>
              <li>The remaining ~7000 subjects used to group SNPs.</li>
            </ul>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> $X$ with dimensions $1771 \times 117933$, consisting of 6403 groups of average size 18.42 (median size 2).</p>
            <br>
            <p><small>
              [1]: Gossmann et. al., 2017.
            </small></p>
            <aside class="notes">
              <p> We applied group SLOPE to the Framingham Heart Study data, which contains SNP data for almost 9000 subjects,...</p>
              <p> ...and we apply group SLOPE to this data matrix X for subjects with BMD measurements. </p>
              <p> We selected SNPs associated with differences in BMD. </p>
            </aside>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE results

* 40 SNPs were selected by Group SLOPE with target gFDR $q = 0.1$, and mapped to nearby genes.
* 15 genes reported in previous studies:
  - BMD (SMOC1, RPS6KA5, FGFR2, GAA, SCN1A, RAB5A, SOX1, and A2BP1),
  - osteoarthritis (A2BP1, ADAM12, MATN1),
  - lumbar disc herniation (KIAA1217),
  - osteopetrosis (VAV3),
  - biology of osteoclasts, osteoblasts and osteogenesis (VAV3, SLC7A7, ADAM12, PPARD, FGFR2, PTPRU, SMOC1).
<aside class="notes">
<p>...</p>
</aside>
            </script>
          </section>

          <section>
            <strong>Group SLOPE &mdash; Some further topics:</strong>
            <ul style="font-size: 0.8em;">
              <li>Performance of SLOPE on DNA seq data.<sup>[1]</sup></li>
              <li>An alternative formulation of Group SLOPE.<sup>[1, 3]</sup></li>
              <li>Theoretical gFDR control under orthogonal groups.<sup>[2-3]</sup></li>
              <li>Error variance estimation: <i>Scaled sparse linear regression</i> vs. <i>EigenPrism</i>.</li>
              <li>Analysis of runtime on large DNA seq datasets.<sup>[3]</sup></li>
              <li>Genomic data preprocessing.<sup>[3]</sup></li>
              <li>Group SLOPE is asymptotically minimax w.r.t. estimation.<sup>[2]</sup></li>
            </ul>
            <p><small>
              [1]: Gossmann et. al., 2015. [2]: Brzyski, Gossmann, et. al., 2018. [3]: Gossmann et. al., 2017.
            </small></p>
            <aside class="notes">
              <p> We have many other results on the group SLOPE method, included in these three papers that do not overlap aside from the basic definition of the method, that are in my thesis but I don't have time to go into today, including its performance on DNA sequence data and mathematical results related to gFDR control, error variance estimation, and estimation properties. </p>
            </aside>
          </section>
        </section>

        <!-- CCA Background -->
        <section>
          <section data-background="#71C5E8">
            <h3>Canonical Correlation Analysis</h3>
            <img width=960 src="./img/20180711-Dissertation-Defense/SCCA.png" alt="(Sparse) Canonical Correlation Analysis">
            <aside class="notes">
              <p>In the previous model we had an outcome variables y and many predictor variables represented by a high-dimensional vector x.</p>
              <p>What if we y is high-dimensional too?</p>
              <p>For example y may represent a subject's fMRI acitvations per brain voxel, while x represents expression values per gene, and we would like to know which genes are related to which brain voxels.</p>
              <p> A popular technique to address this is CCA. </p>
              <p> CCA finds a linear combination of the fMRI features and a linear combination of the genetic features as two new variables with as large a correlation as possible. This amounts to estimating two coefficient vectors u and v, which are referred to as canonical vectors. Xu and Yv are called canonical variates. </p>
              <p> In practice, we can apply further constraints on u and v, like sparsity. </p>
            </aside>
          </section>

          <section>
            <h3>Classical CCA [Hotelling, 1936]</h3>
            <p>Consider two matrices (i.e., datasets)<br>$X\in\mathbb{R}^{n\times p}$ and $Y\in\mathbb{R}^{n \times q}$ (with columns centered).</p>
            <p>$$\mathrm{maximize}\subscript{u\in\mathbb{R}^p, v\in\mathbb{R}^q} \widehat{\mathrm{Cov}}(Xu, Yv) = \frac{1}{n} u^T X^T Y v,$$
            $$\mathrm{subject\,to} \quad \widehat{\mathrm{Var}}(Xu) = 1, \widehat{\mathrm{Var}}(Yv) = 1.$$ </p>
            <p class="fragment grow">The problem is degenerate if $n \leq \mathrm{max}\left( p, q \right)$.<br>+ Sparsity assumption on biological data</p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>Sparse CCA<sup>[1-2]</sup></h3>
            <p>$\mathrm{maximize}\subscript{\mathbf{u}\in\mathbb{R}^p, \mathbf{v}\in\mathbb{R}^q} \frac{1}{n} \mathbf{u}^T X^T Y \mathbf{v},$</p>
            <p>subject to</p>
            <p>$\lVert \mathbf{u} \rVert_2^2 \leq 1, \lVert \mathbf{v} \rVert_2^2 \leq 1, \lVert \mathbf{u} \rVert_1 \leq c_1, \lVert \mathbf{v} \rVert_1 \leq c_2$.</p>
            <ul>
              <li>Unique solution even when $p_X, p_Y \gg n$.</li>
              <li class="fragment grow">Selection of the sparsity parameters remains a challenging problem.</li>
            </ul>
            <br><br>
            <p><small>[1]: Witten et. al., 2009, [2]: Parkhomenko et. al., 2009.</small></p>
            <aside class="notes">
              <p> Sparse CCA methods have been proposed to provide a way to account for the sparsity assumption, as well as to ensure a unique solution even when the sample size is smaller than the number of variables. </p>
              <p> The model is similar to the classical CCA, with an L1 penalty put onto u and v. </p>
              <p> BUT it's unclear how to set the sparsity of the solution, which is controlled by the penalty parameters c1 and c2. </p>
            </aside>
          </section>
        </section>

        <!-- Sparse CCA with FDR control -->
        <section>
          <section>
            <h3>Sparse CCA</h3>
            <p><i class="em em-arrow_right"></i> Select sparsity parameters in a data-driven fashion, such that FDR is controlled.</p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>Defining FDR for sparse CCA</h3>
            <ul>
              <li>Adapt the widely-used FDR concept from multiple hypotheses testing to CCA.<sup>[1]</sup>
                <blockquote>
                The expected proportion of &ldquo;discoveries&rdquo; that are false.
                </blockquote>
              </li>
              <li>Consider FDR in $\mathbf{u}$ and in $\mathbf{v}$ separately.</li>
            </ul>
            <br>
            <br>
            <p><small>[1]: Benjamini &amp; Hochberg, JRSSB, 1995.</small></p>
            <aside class="notes">
              <p> In order to define the FDR, which is the expected proportion of discoveries that are false, for sparse CCA, we consider the FDR for u and v spearately since they come from two different types of data, and we need to consider what it means to have a false discovery in the context of CCA. </p>
            </aside>
          </section>

          <section>
            <h3>Defining FDR for sparse CCA</h3>
            <blockquote>
              The coefficient estimate $\hat{u}_i \neq 0$ represents a <b>false discovery</b> of the $i$th feature of $X$, if $u_i$ doesn't affect the value of $\mathrm{Cov}(\mathbf{x} \cdot \mathbf{u}, \mathbf{y} \cdot \mathbf{v})$, or <i>equivalently</i> if
              $$\hat{u}_i \neq 0 \quad \text{and} \quad \mathrm{E}(X^T Y \mathbf{v})_i = 0.$$
            </blockquote>
            <aside class="notes">
              <p> Since CCA aims to maximize the covariance between Xu and Yv, we consider u_i to be a false discovery if it's nonzero and does not affect the valu eof the covariance. You can show that this is equivalent to this expected value being equal to zero. </p>
            </aside>
          </section>

          <section>
            <h3>Defining FDR for sparse CCA</h3>
            <ul>
              <li>$\mathrm{R}\subscript{\hat{\mathbf{u}}} =$ the number of non-zero elements in $\hat{\mathbf{u}}$,</li>
              <li>$\mathrm{V}\subscript{\hat{\mathbf{u}}} =$ the number of false discoveries.</li>
            </ul>
            <p>Define</p>
            <p>$\mathrm{FDR}(\hat{\mathbf{u}}) := \mathbb{E}\left( \frac{\mathrm{V}\subscript{\hat{\mathbf{u}}}}{\max\left\{ \mathrm{R}\subscript{\hat{\mathbf{u}}}, 1 \right\}} \right).$</p>
            <aside class="notes">
              <p> Then we define the FDR similarly to before, as the expected value of the number of false discoveries over the number of non-zero elements in u.</p>
            </aside>
          </section>

          <section>
            <h3>Defining gFDR for sparse CCA</h3>
            <p>Analogously we define the group-wise false discovery rate (gFDR).</p>
            <ul>
              <li>$\mathrm{Rg}\subscript{\hat{\mathbf{u}}} =$ the number of non-zero groups of elements in $\hat{\mathbf{u}}$,</li>
              <li>$\mathrm{Vg}\subscript{\hat{\mathbf{u}}} =$ the number of falsely discovered groups.</li>
            </ul>
            <p>Define</p>
            <p>$\mathrm{gFDR}(\hat{\mathbf{u}}) := \mathbb{E}\left( \frac{\mathrm{Vg}\subscript{\hat{\mathbf{u}}}}{\max\left\{ \mathrm{Rg}\subscript{\hat{\mathbf{u}}}, 1 \right\}} \right).$</p>
            <aside class="notes">
              <p> We can then analogously define the groupwise FDR. </p>
            </aside>
          </section>
        </section>

        <!-- SlopeCCA and gSlopeCCA -->
        <section>
          <section data-background="#D0DEBB">
            <h1>SlopeCCA and gSlopeCCA</h1>
            <aside class="notes">
              <p>Having the FDR and gFDR definition in place, we propose three different sparse CCA methods that keep the FDR or gFDR below a user-specified level.</p>
              <p> The first two work on data with block diagonal covariance structures. The third put no assumptions on the covariance structures and is intended for sparser settings, but has a lower detection power. </p>
            </aside>
          </section>

          <section>
            <p><strong>slopeCCA:</strong></p>
            <p><small>
              $\mathrm{minimize}\subscript{\fatu\in\R^p, \fatv\in\R^q}\, \left\{ -\fatu^T X^T Y \fatv + \sqrt{n} J\subscript{\fatlambda^u}(\fatu) + \sqrt{n} J\subscript{\fatlambda^v}(\fatv) \right\}$,
              <br>
              subject to $\lVert\fatu\rVert\subscript{2}^2 \leq 1, \lVert\fatv\rVert\subscript{2}^2 \leq 1$.
            </small></p>
            <p><strong>gslopeCCA:</strong></p>
            <p><small>
              $\mathrm{minimize}\subscript{\fatu\in\R^p, \fatv\in\R^q} \left\{ -\fatu^T X^T Y \fatv + \sqrt{n} J\subscript{\fatlambda^u}\left( \left( \lVert \fatu\subscript{1} \rVert\subscript{2}, \dots\right)^T \right) + \sqrt{n} J\subscript{\fatlambda^v}\left(  \left( \lVert \fatv\subscript{1} \rVert\subscript{2}, \dots \right)^T \right) \right\}$,
              subject to $\lVert\fatu\rVert\subscript{2}^2 \leq 1, \lVert\fatv\rVert\subscript{2}^2 \leq 1$.
            </small></p>
            <p><small>Where $J\subscript{\fatlambda}(\fatu) = \Sigma\subscript{i=1}^p \lambda\subscript{i} |u|\subscript{(i)}$ is the Sorted L1 Norm.</small></p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>SlopeCCA and gslopeCCA - computational methods</h3>
            <p>Both are <strong>biconvex optimization problems</strong>.</p>
            <p><strong>Alternating optimization</strong> algorithms are guaranteed to converge.</p>
            <aside class="notes">
              <p>Again, for the methods to be useful in practice, we need to know how to obtain the solution efficiently.</p>
            </aside>
          </section>

          <section>
            <h3>SlopeCCA and gSlopeCCA - theoretical guarantees</h3>
            <p>Asymptotic (i.e., as $n\to\infty$) FDR and gFDR guarantees if $Cov(X)$ and $Cov(Y)$ are diagonal or block-diagonal.</p>
            <p>($Cov(X, Y)$ can be of arbitrary shape)</p>
            <aside class="notes">
              <p> We provide theoretical guarantees for the FDR as the sample size gets large, as long as Cov(x) and Cov(y) are block-diagonal. </p>
            </aside>
          </section>

          <section>
            <img width="800" src="./img/20180711-Dissertation-Defense/slopeCCA_FDR.png" alt="Simulation studies investigating the false discovery rates resulting from slopeCCA, gslopeCCA, and L1-penalized sparse CCA.">
            <p><small><strong>Simulation studies:</strong> FDR and gFDR of slopeCCA, gslopeCCA, and $\ell_1$-penalized CCA solutions; $n = 200, 1000, 5000, 10000$ and $p_X = p_Y = 300$; 11 evenly spaced sparsity levels between 0 (i.e., $X$ is uncorrelated with $Y$) and 1 (i.e., every feature of $X$ is correlated to some feature of $Y$, and vice versa); 500 independent simulation runs; dashed line represents theoretical bounds for slopeCCA and gslopeCCA.<br>[Figure 4.1 in the thesis]</small></p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section data-markdown>
            <script type="text/template">
### Application to TCGA

The Cancer Genome Atlas (TCGA):

* NIH initiative since 2005.
* Coordinated data collection at 20 collaborating institutions in U.S. and Canada.
* Genomic samples from tumor cells of different cancers and matched normal cells.
* Data include: gene expression, methylation, CNV, SNP, microRNA, and whole genome, exon, or transcriptome sequencing.
* <https://cancergenome.nih.gov/>
<aside class="notes">
<p>...</p>
</aside>
            </script>
          </section>

          <section>
            <img width="700" src="./img/20180711-Dissertation-Defense/TCGA_gslopeCCA_canonical_variates.png" alt="Results from an application of gSlopeCCA to the Cancer Genome Atlas data">
            <p><small>Canonical variates estimated by gslopeCCA on methylation and mRNA data from TCGA ($n = 1109$, $p_X = 24981$, $p_Y = 20255$).
              Observations are colored according to cancer type, revealing clear differences.
              <br>[Figure 4.2 in the thesis]</small></p>
            <aside class="notes">
              <p> We can use gslopeCCA to look at the canonical variates, reducing the number of variables we need to consider from over 40,000 to just 4. </p>
              <p> Plotting these 4 against each other, and coloring the points based on the cancer type of the corresponding observation, we see that these four derived variables in fact separate the difference cancer types very well. </p>
              <p> This implies that this method can be used in diagnosis, and to reveal new information about disease phenotypes. </p>
            </aside>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### TCGA &mdash; Conclusions

* Canonical variates reveal differences between cancer types.
* Using the four canonical variates as the only predictors to classify cancer type yields classification accuracy over 93% on test data.
<aside class="notes">
<p>...</p>
</aside>
            </script>
          </section>

          <section>
            <h3>SlopeCCA and gslopeCCA</h3>
            <p><b>Some further topics covered in the thesis:</b></p>
            <ul>
              <li>Optimization algorithms.</li>
              <li>Asymptotic normality results.</li>
              <li>Genomic data pre-processing.</li>
            </ul>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>
        </section>

        <!-- FDR-corrected SCCA -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/T2star_preproc_facetted.gif">
            <h1>FDR-corrected sparse CCA</h1>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h2>FDR-corrected sparse CCA</h2>

            <p><b>Motivation:</b> The assumption of slopeCCA and gslopeCCA that $Cov(X)$ and $Cov(Y)$ are diagonal or block-diagonal is too restrictive in many cases.</p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h2>FDR-corrected sparse CCA</h2>
            <p>A split-sample, two-step procedure:<sup>[1]</sup></p>
            <ol>
              <li>Split the data in two parts.</li>
              <li><strong>Using the first subsample:</strong> obtain initial estimates $\hat{\fatu}^{(0)}$ and $\hat{\fatv}^{(0)}$ using conventional sparse CCA.</li>
              <li><strong>Using the second subsample:</strong> test hypotheses of the form,
                $$\mathrm{H}^{(u)}\subscript{i} : \mathrm{E}\left(X^T Y \mathbf{v}\right)\subscript{i} = 0, \quad \mathrm{H}^{(v)}\subscript{j} : \mathrm{E}\left(Y^T X \mathbf{u}\right)\subscript{j} = 0,$$
                and adjust for multiple comparisons to control FDR.
              </li>
            </ol>
            <br>
            <br>
            <p><small>[1]: Gossmann et. al., IEEE TMI, 2018.</small></p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>FDR-corrected sparse CCA - Theoretical FDR Guarantees</h3>
            <ol>
              <li>Asymptotic theory allows us to approximate the distributions of $X^T Y \hat{\fatv}^{(0)}$ and $Y^T X \hat{\fatu}^{(0)}$.</li>
              <li>We can use the Benjamini-Hochberg procedure to control the FDR.</li>
            </ol>
            <p class="fragment"><i class="em em-arrow_right"></i> FDR control confirmed with extensive simulation studies on synthetic and real data.<sup>[1]</sup>
            <br>
            <br>
            <small>[1]: Gossmann et. al., IEEE TMI, 2018.</small></p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>Simulation Studies</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/high_dim_FDR_combined.png" alt="Simulation studies of the FDR-corrected sparse CCA method.">
            <p>$X, Y \in \R^{600 \times 1500}$.
            FDR is controlled regardless of the sparsity of the true solution.</p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>Simulation Studies</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/high_dim_TPR_combined.png" alt="Simulation studies of the FDR-corrected sparse CCA method.">
            <p>True positive rate is on par with competing methods.</p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>FDR-corrected SCCA - Application</h3>
            <ul>
              <li>Diversity in brain activity and brain connectivity in children and adolescents.</li>
              <li>What are the driver genes?</li>
              <li>Relationship to neurodevelopmental and psychiatric disorders.</li>
            </ul>
            <aside class="notes">
              <p> One application of FDR corrected SCCA is studying the diversity in brain activity and connectivity in children and adolescents in relationship to their genomic information to answer questions like "What are the driver genes behind certain neurodevelopmental and psychiatric disorders?"</p>
            </aside>
          </section>

          <section>
            <h3>Dataset</h3>
            <blockquote>
              The Philadelphia Neurodevelopmental Cohort (PNC) is a large-scale collaborative study between the Brain Behaviour Laboratory at the University of Pennsylvania and the Children's Hospital of Philadelphia. It contains a fractal $n$-back fMRI task, an emotion identification fMRI task, SNP arrays, and questionnaire data for over 900 adolescents.
            </blockquote>
            <aside class="notes">
              <p>Including subjects with increased (prodromal) symptoms of ADD (107 PNC subjects), schizophrenia (103 PNC subjects), and depression (85 PNC subjects) [Kaufmann et. al., 2017].</p>
            </aside>
          </section>

          <section>
            <h3>Objective</h3>

            <p>Use sparse CCA to identify the relationships between brain activity, brain connectivity, and genomics.</p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>Application 1: n-back fMRI vs. SNPs</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/voxels_vs_genes.png" alt="PNC results: n-back task fMRI vs. SNP analysis">
            <p><small>Selection from $85796$ brain voxels and $60372$ genomic features. [Figure 5.5 in the thesis]</small></p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h4>Results validation - n-back fMRI vs. SNPs</h4>
            <img width="260" src="./img/20180711-Dissertation-Defense/voxels_vs_genes.png" alt="PNC results: n-back task fMRI vs. SNP analysis">
            <ol>
              <li>Similar brain regions have been found in other fMRI studies of working memory.</li>
              <li>At least 34 out of the 65 identified genes have been previously associated with various aspects of human cognitive function.</li>
            </ol>
            <aside class="notes">
              <p> More than half of the genes we identified have been previously associated with human cognitive function, supporting the biological validity of our results, and indicating that the remaining identified genes may provide valuable biological hypotheses that should be followed up with additional studies.</p>
            </aside>
          </section>

          <section>
            <h3>Application 2: Functional connectivity (FC) vs. SNPs</h3>
            <ol>
              <li>Emotion identification task fMRI data transformed to FC measures.</li>
              <li>FDR-corrected sparse CCA solution includes 129 genomic features and 107 FC features.</li>
            </ol>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h4>FC vs. SNPs - Top 10 selected genes</h4>
            <table style="font-size: 0.6em;">
              <thead>
                <tr>
                  <th>Gene</th>
                  <th>Previously studies in association with...</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>DAB1   </td>
                  <td>Autism, schizophrenia, brain development</td>
                </tr>
                <tr>
                  <td>NAV2   </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>WWOX   </td>
                  <td>Cognitive ability, brain development</td>
                </tr>
                <tr>
                  <td>CNTNAP2</td>
                  <td>Autism, brain connectivity, brain development, schizophrenia, major depression, cognitive ability (linguistic processing)</td>
                </tr>
                <tr>
                  <td>NELL1  </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>PTPRT  </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>FHIT   </td>
                  <td>Cognitive ability, autism, ADHD</td>
                </tr>
                <tr>
                  <td>MACROD2</td>
                  <td>Autism</td>
                </tr>
                <tr>
                  <td>LRP1B  </td>
                  <td>Cognitive function</td>
                </tr>
                <tr>
                  <td>DGKB   </td>
                  <td>Brain development, bipolar disorder</td>
                </tr>
              </tbody>
            </table>
            <p><small>(for detail see [Gossmann et. al., TMI, 2018])</small></p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h3>FDR-corrected sparse CCA</h3>
            <p style="font-size: 0.8em;"><b>Some further topics:</b></p>
            <ul style="font-size: 0.8em;">
              <li>Further simulation studies with other underlying covariance structures.<sup>[1]</sup></li>
              <li>Further simulation studies on real DNA sequence data.<sup>[1]</sup></li>
              <li>Preprocessing steps for the genomic and the fMRI data.<sup>[1]</sup></li>
              <li>Exploratory analysis, and analysis of confounding factors in the data.<sup>[1]</sup></li>
            </ul>
            <br>
            <p><small>
              [1]: Gossmann et. al., IEEE TMI, 2018.
            </small></p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>
        </section>

        <!-- Thresholdout -->
        <section>
          <section data-background="#78BE20">
            <h3>Another type of false findings</h3>
            <p><i class="em em-white_check_mark"></i> Feature selection with FDR control.</p>
            <p><i class="em em-arrow_right"></i> Features can be used to fit a predictive model.</p>
            <p class="fragment grow">Danger of over-fitting to the local noise in the given dataset, resulting in false predictions on new data.</p>
            <p>What to do?</p>
            <aside class="notes">
              <p>Now that we know how to identify truly relevant predictive features, we can come back to the problem of prediction.</p>
              <p>Whenever one does prediction based on data, there is the danger of obtaining a model that is extremely accurate on the data that you are using, but which does not generalize to the population from which the data were drawn.</p>
              <p>This is called over-fitting in statistics and machine learning.</p>
              <p>A model that is overfit to a specific dataset will yield many FP on new data when the reported performance of the model is much better.</p>
            </aside>
          </section>

          <section>
            <p>In Machine Learning practice, generally, usage of two independent datasets &mdash; <i>"training"</i> and <i>"test"</i> data.</p>
            <p class="fragment"><strong>Training data:</strong> exploratory analysis, model fitting, parameter tuning, comparison of different machine learning algorithms, feature selection, etc.</p>
            <p class="fragment fade-right">$\Longrightarrow$ Adaptive machine learning, risk of overfitting.</p>
            <p class="fragment"><strong>Test data:</strong> Performance evaluation <i>after the trained machine learning algorithm has been "frozen"</i>.</p>
            <p class="fragment fade-right">$\Longrightarrow$ Accurate performance measures of the final model, <strong>if the test data is used only once</strong>.</p>
              <aside class="notes">
                <p>The performance metric obtained on test data will make it evident whether any overfitting has occurred during model training.</p>
                <p>Other techniques (multiple testing, cross-validation, bootstrap) can be used to avoid or reduce the overfitting on the training data.</p>
              </aside>
          </section>

          <section data-transition="slide-in fade-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_1.png" alt="Test data reuse 1">
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section data-transition="fade-in fade-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_2.png" alt="Test data reuse 2">
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section data-transition="fade-in slide-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_3.png" alt="Test data reuse 3">
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>

          <section>
            <h2>Idea</h2>
            <p>Can we <strong>obfuscate</strong> the test data to avoid overfitting?</p>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> Differential privacy.<sup>[1]</sup></p>
            <p class="fragment">Promising simulation results.<sup>[2-3]</sup>
            <br><br><br>
            <small>[1]: Dwork, McSherry, Nissim, Smith, 2006.<br> [2]: Dwork et. al., Science, 2015.<br> [3]: Gossmann et. al., SPIE 2018.</small></p>
            <aside class="notes">
              <p> We would like to be able to reuse the test data while avoiding overfitting by somehow obfuscating the test data. </p>
              <p> The idea of concealing the data is intuitively related to data privacy. </p>
              <p> One way to do this is through the use of differential privacy...</p>
              <p> ...which has shown promising simulation results.</p>
            </aside>
          </section>

          <section>
            <h3>Differential privacy (Dwork, McSherry, Nissim, Smith, 2006)</h3>

            <ul>
              <li>A mathematically rigorous definition of data privacy.
                  $$P[\mathcal{M}(D) \in S] \leq e^\varepsilon P[\mathcal{M}(D^\prime) \in S] + \delta.$$
              </li>
              <li><b>Idea:</b> An individual data point has little impact on the value reported by a DP mechanism.</li>
              <li><b>Purpose:</b> An adversary cannot learn an individual data point from querying a DP mechanism.</li>
              <li><b>Properties:</b> DP is <i>preserved</i> under <i>post-processing</i> and under <i>adaptive composition</i>.</li>
            </ul>
            <aside class="notes">
              <p>Differential privacy is a beautiful mathematically rigorous definition of privacy, that has gained increasing attention in about the last 5 years (in fact, Cynthia Dwork and co-authors have received the Goedel price for the invention of DP last year, which is the highest price in theoretical computer science, named after the logician Kurt Goedel, who is known for his incompleteness theorems).</p>
              <p> The idea behind DP is ... so that an adversary cannot learn an individual data point. We can apply this idea to test data reuse where a machine learning algorithm will not be able to learn individual records within the test data set and so will not be able to overfit to them.</p>
              <p> DP is preserved under post-processing and adaptive composition, which means that it can be applied to the type of adaptive data analysis process that we are dealing with.</p>
            </aside>
          </section>

          <section data-markdown>
            <script type="text/template">
### Differentially private access to test data

Currently available literature:

* Focuses on theory.
* Theoretical assumptions are too restrictive for most of applied data analysis and machine learning.
* Computational experiments are rather simplistic.

<aside class="notes">
  <p>...</p>
</aside>
            </script>
          </section>

          <section style="text-align: left;">
<img src="./img/20180211-SPIE-Medical-Imaging/ROC.png" style="float: right;" alt="Figure of an ROC curve">
<p><strong>Thresholdout + AUC = <i class="em em-heart"></i></strong></p>
<p>Thresholdout$_{\mathrm{AUC}}$<sup>[2]</sup> combines the Thresholdout technique<sup>[1]</sup> with AUC as the reported performance metric on test data.</p>
<br>
<p><small>[1]: Dwork et. al., Science, 2015.<br>[2]: Gossmann et. al., SPIE 2018.</small></p>
<aside class="notes">
  <p> In order to report the performance on the test data in a differentially private way, we consider the Thresholdout technique which can return some simple kinds of performance metrics, and modify it in order to obtain the area under the ROC curve, which relates the true positive and false positive rates. The AUC is the recommended performance metric in many fields of medical research.</p>
  <ul>
    <li>Invariance to prevalence.</li>
    <li>Independence of the decision threshold (can be chosen later).</li>
    <li>Probabilistic meaning.</li>
    <li>Extensively used in the medical field, including medical imaging.</li>
  </ul>
</aside>
          </section>

          <section>
            <h3>Thresholdout$_{\mathrm{AUC}}$ &mdash; rough summary</h3>

            <div style="background-color: rgba(0, 0, 0, 0.8); color: #fff; padding: 10px; width: 600px; margin: auto;">
              Trained classifier $\phi(x) \in [0,1]$
            </div>

            <p><i class="em em-arrow_down"></i></p>

            <div style="background-color: rgba(0, 0, 0, 0.8); color: #fff; padding: 10px; width: 800px; margin: auto;">
              <p style="text-align: left;">
                <b>If</b> $\lvert \mathrm{AUC}_{\mathrm{training}}(\phi) - \mathrm{AUC}_{\mathrm{test}}(\phi) \rvert > \tilde{T}$:<br>
                &nbsp;&nbsp;&nbsp;&nbsp;output $\mathrm{AUC}_{\mathrm{test}}(\phi) +$ "a little noise"<br>
                <b>Else</b>:<br>
                &nbsp;&nbsp;&nbsp;&nbsp;output $\mathrm{AUC}_{\mathrm{training}}(\phi)$
              </p>
            </div>
            <aside class="notes">
              <p> Essentially, our ThresholdoutAUC mechanism will return the AUC on the test data plus some noise if this performance metric on the test data and training data is significantly different, and otherwise will return the AUC on the training data and not provide any information on the test data at all.</p>
            </aside>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### Theorem &mdash; rough summary

              1. **If** a test dataset, which is used for performance evaluation repeatedly, is only accessed via Thresholdout$_{\mathrm{AUC}}$.
              $\Longrightarrow$ **Then** with a high probability $(1 − \beta)$ the reported AUC estimates will be correct up to a small tolerance $\tau$.
              2. **Restriction**: Test data access "budget" $B$, which is linear in the size of the test data $n$, and also depends on $\beta$, $\tau$, and the class balance.

              <aside class="notes">
                <p>$\beta$ and $\tau$ are pre-specified by the analyst.</p>
                <p>This result holds even when each analysis step is adaptively chosen based on the reported AUC estimates obtained from previous analyses on the same test data using Thresholdout$_{\mathrm{AUC}}$.</p>
              </aside>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### Logistic Regression

              ![Simulation results](./img/20180211-SPIE-Medical-Imaging/naive_holdout_reuse_vs_thresholdout_AUC_landscape_GLM_only.png)
            <aside class="notes">
              <p>...</p>
            </aside>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### Logistic Regression

              * __Naive approach__: Classifier learns the effect of local noise in the test data (overfitting).
              * __Thresholdout approach__: The gap between the reported and the true AUC is much narrower!

              ![Simulation results](./img/20180211-SPIE-Medical-Imaging/naive_holdout_reuse_vs_thresholdout_AUC_landscape_GLM_only_small.png)

              <aside class="notes">
                <p>We may reuse the test data as often as we like, with the constraint that only AUC values can be reported back from the test data.</p>
                <p>What model to try next is determined by the AUC values obtained on the test data for the previous models.</p>
              </aside>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
              ![Simulation results](./img/20180211-SPIE-Medical-Imaging/naive_holdout_reuse_vs_thresholdout_AUC_landscape.png)
              <small>Accuracy of reported AUC values is improved, at the cost of slightly higher uncertainty in the reported AUC, and slightly worse predictive performance.<br>[Figure 6.1 (a) in the thesis]</small>
            <aside class="notes">
              <p>...</p>
            </aside>
            </script>
          </section>

          <section>
            <h3>Thesholdout<sub>AUC</sub></h3>
            <p><b>Some further topics covered in the thesis:</b></p>
            <ul>
              <li>AUC &mdash; discussion and benefits.</li>
              <li>Complete statement of the Thesholdout<sub>AUC</sub> procedure.</li>
              <li>Data generation.</li>
              <li>Simulation procedure detail.</li>
              <li>Choice of the tuning parameters within Thesholdout<sub>AUC</sub>.</li>
              <li>Analysis of the AUC estimation error.</li>
            </ul>
            <br>
            <p><small>
              [1]: Gossmann et. al., SPIE Medical Imaging 2018.
            </small></p>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>
        </section>

        <section>
          <section data-background="#093748">
            <p><strong>Parts of this work appear in:</strong></p>
            <ol>
              <li><strong>G.A.</strong>, Cao, S., &amp; Wang, Y.-P. In proceedings of ACM BCB '15. 2015.</li>
              <li><strong>G.A.</strong>, Cao, S., Brzyski, D., Zhao, L. J., Deng, H. W., &amp; Wang, Y. P. IEEE/ACM TCBB. 2017.</li>
              <li>Brzyski, D., <strong>G.A.</strong>, Su, W., &amp; Bogdan, M. JASA. 2018.</li>
              <li><strong>G.A.</strong>, Zille, P., Calhoun, V., &amp; Wang, Y.-P. IEEE TMI. 2018.</li>
              <li><strong>G.A.</strong>, Pezeshk, A., &amp; Sahiner, B. In proceedings of SPIE Medical Imaging '18. 2018.</li>
            </ol>
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>
        </section>

        <!-- Last slide -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/dna-with-equations.jpg">
            <img width="600" src="./img/20180711-Dissertation-Defense/T2star_preproc_facetted_questions.gif" alt="Structural MRI animation">
            <aside class="notes">
              <p>...</p>
            </aside>
          </section>
        </section>

      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>

      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // MathJax integration
        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'
        },

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/search/search.js', async: true },
          // Zoom in and out with Alt+click
          { src: 'plugin/zoom-js/zoom.js', async: true },
          // Speaker notes
          { src: 'plugin/notes/notes.js', async: true },
          // MathJax
          { src: 'plugin/math/math.js', async: true }
        ]
      });

    </script>

  </body>
</html>
