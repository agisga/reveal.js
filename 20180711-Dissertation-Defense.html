<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Bioinnovation PhD Program – Dissertation Defense - Alexej Gossmann</title>

    <meta name="description" content="Bioinnovation PhD Program - Dissertation Defense">
    <meta name="author" content="Alexej Gossmann">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Emoji support -->
    <!-- See: https://afeld.github.io/emoji-css/ -->
    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!-- MathJax macros (inline math delimiters, new commands, etc.), more configuration under Reveal.initialize below -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ["$","$"] ],
          displayMath: [ ["$$","$$"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: { autoNumber: "AMS" },
          Macros: {
            subscript: ['_{#1}', 1],
            superscript: ['^{#1}', 1],
            fatu: '\\mathbf{u}',
            fatv: '\\mathbf{v}',
            fatlambda: '\\boldsymbol{\\lambda}',
            R: '\\mathbb{R}'
          }
        }
      });
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h3>Regaining control of false findings in feature selection, classification, and prediction on neuroimaging and genomics data</h3>
          <p>
          <small>Oral defense of a dissertation submitted to the <b>Bioinnovation PhD Program</b> of the <b>School of Science and Engineering</b> of <b>Tulane University</b> in partial fulfillment of the requirements for the PhD degree by</small>
          </p>
          <h4>Alexej Gossmann</h4>
          <p>July 11, 2018</p>
          <aside class="notes">
            <p>I will present to you my dissertation work on machine learning methodology for neuroimaging genomics.</p>
            <p>I won't have the time to go much into detail, since I have many projects to discuss.
            But I will be happy to answer any questions you have after the talk.</p>
          </aside>
        </section>

        <!-- Precision Medicine -->
        <section>
          <section>
            <h2>Precision Medicine</h2>
            <p>Inter-personal diversity in the patients' biology</p>
            <p class="fragment fade-right">⇾ "personalized" treatment plans</p>
            <p class="fragment fade-up">↑ Quality of healthcare</p>
            <p class="fragment fade-down">↓ Treatment time and cost</p>
            <aside class="notes">
              <p>As discussed in some detail during my prospectus defense the motivation and the ultimate purpose of this research is precision medicine.</p>
              <p>⇾ differences in disease susceptibility/progression</p>
              <p>⇾ differences in treatment efficacy</p>
              <p>⇾ "personalized" treatment plans.</p>
              <p>New drugs and devices targeting specific subpopulations (or even individuals).</p>
              <p>No more treatment based on trial-and-error.</p>
            </aside>
          </section>

          <section>
            <h2>Precision Medicine</h2>
            Made possible by:
            <ol>
              <li class="fragment highlight-green">Big data including <b>genomics</b> and <b>neuroimaging</b>.</li>
              <li class="fragment highlight-red">Computational methods including <b>machine learning</b> and <b>modern statistics</b>.</li>
            </ol>
          </section>
        </section>

        <!-- Review of genomics and neuroimaging -->
        <section>
          <section data-background="#0f829d">
            <small>From left to right: (1) gene region on a chromosome; (2) chemical structure of DNA; (3) transcription/translation of genes into ncRNA, mRNA, protein.</small>
            <img width="380" src="./img/20180711-Dissertation-Defense/Chromosome_DNA_Gene.svg.png" alt="Gene region on a chromosome">
            <img width="500" src="./img/20180711-Dissertation-Defense/DNA_chemical_structure_2.svg.png" alt="Chemical structure of genes">
            <img width="390" src="./img/20180711-Dissertation-Defense/DNA_to_protein_or_ncRNA.svg.png" alt="Transcription and translation of genes">
            <br>
            <small>Source: Images by Thomas Shafee [<a href="http://creativecommons.org/licenses/by/4.0">CC BY 4.0</a>] via Wikimedia Commons.</small>

            <aside class="notes">
              <p>First let me give you some background on where genomic and neuroimagning data comes from.
              A gene is a section of DNA.
              DNA is physically encoded in four molecules (nucleobases).
              Broadly speaking, genes encode proteins or non-coding RNA, which have a biological function.
              Genomic data include information on the structure of the DNA, and amount of protein produced, which can all be expressed/stored in numeric form.
            </aside>
          </section>

          <section data-background="#0f829d">
            <ul>
              <li>Structural MRI: anatomical structure of the brain.</li>
              <li>Functional MRI: brain activity associated with blood flow related to energy use by brain cells across time.</li>
            </ul>
            <img width="270" src="./img/20180711-Dissertation-Defense/T1.gif" alt="Structural MRI animation">
            <img width="270" src="./img/20180711-Dissertation-Defense/T2star_raw.gif" alt="Raw BOLD fMRI (n-back task) from a randomly chosen PNC subject at a fixed time point">
            <img width="270" src="./img/20180711-Dissertation-Defense/T2star_preproc.gif" alt="SPM12 pre-processed BOLD fMRI (n-back task) from a randomly chosen PNC subbject at a fixed time point">
            <small>
            <ul>
              <li>
                A randomly chosen subject from the Philadelphia Neurodevelopmental Cohort:
                <ul>
                  <li>T1-weighted MRI before preprocessing ($192\times 256 \times 160$ voxels).</li>
                  <li>n-back task BOLD fMRI (i.e., T2*-weighted) before preprocessing ($64\times 64\times 49$ voxels), and after preprocessing ($79\times 95\times 79$ voxels).</li>
                </ul>
              </li>
            </ul>
            </small>
            <aside class="notes">
              <p>These are some MRIs of the human brain.</p>
              <p>They either supply us with anatomical structure, or brain activity over time.</p>
              <p>MRI 3D volumes are collected as 2D axial slices, as I have visualized in the shown animations for a randomly chosen PNC subject (about 10 years old, female). As discussed in the thesis there</p>
              <p>You can see a raw structural or T1-weighted MRI on the left, a raw BOLD fMRI in the middle, and an fMRI after preprocessing on the right.</p>
              <p>I use BOLD fMRI data which gives you brain activity measures in brain voxels, which are 3d pixels, at discrete points in time.</p>
              <p>In this thesis I mostly use BOLD fMRI data which gives you brain activity measures in brain voxels, which are 3d pixels, at discrete points in time.</p>
            </aside>
          </section>

          <section data-background="./img/20180711-Dissertation-Defense/T2star_preproc_facetted.gif" data-background-size="900px" data-background-color="#0f829d">
            <p>[230 time points]</p>
            <aside class="notes">
              <p>Here you see the data for the same random subject visualized at different time points.</p>
            </aside>
          </section>

          <section>
            <h3>Precision Med. & Mental Disorders</h3>
            <img width="600" src="./img/20180711-Dissertation-Defense/endophenotype.png" alt="Relationship: Genomics - Brain MRI - Phenotype">
            <ul>
              <li>Neuroimaging as an endophenotype.<sup>[1-2]</sup></li>
              <li>Use of fMRI to aid diagnosis, or to monitor &amp; guide drug treatment.<sup>[3-5]</sup></li>
            </ul>
            <br><br>
            <p><small>[1]: Hashimoto et. al., 2015, [2]: Poline et. al., 2015, [3]: Weickert et al., 2004, [4]: Apud et al., 2007, [5]: Goldstein-Piekarski et al., 2016.</small></p>
            <aside class="notes">
              <p>To bring this back to the topic of precision medicine:</p>
              <p>Mental disorders are known to be heritable, but it has proven to be very difficult to identify the causal genes of these complex diseases.
              Measures of brain function and structure, as an intermediate phenotype between genomics and clinical traits or behavioral measures, are more powerful in the identification of genomic associations, and in the interpretation of the function of the identified genes
              (because gene expression directly affects cellular metabolism, which directly influences the neural circuity).</p>
            </aside>
          </section>
        </section>

        <!-- 2 types of problems: feature selection and prediction -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/catwalk.gif" data-background-repeat="repeat" data-background-size="400px">
            <div style="background-color: rgba(0, 0, 0, 0.8); color: #fff; padding: 40px;">
              <h1>Models</h1>
            </div>
            <aside class = "notes">
              <p>Before I present my methods and results, let me give you some brief backgournd on statistical models.</p>
            </aside>
          </section>

          <section>
            <h2>Models</h2>
            <p>
            $$y = f(x\subscript{1}, x\subscript{2}, \ldots, x\subscript{p}) + \varepsilon,$$
            where $x\subscript{1}, x\subscript{2}, \ldots, x\subscript{p}$ are predictor variables, $\varepsilon$ is random noise, and $y$ is the phenotype.</p>
            <p class="fragment fade-up">Human DNA $\approx 3\cdot 10^9$ base pairs ⇝ vast majority not related to phenotype of interest
            <i class="fragment highlight-red">⇝ sparse models</i>
            $$\Rightarrow y = f(x\subscript{a\subscript{1}}, x\subscript{a\subscript{2}}, \ldots, x\subscript{a\subscript{m}}) + \varepsilon,$$
            where $\{a\subscript{1}, a\subscript{2}, \ldots, a\subscript{m}\} \subset \{1,2,\ldots,p\}$ is a small subset ($m \ll p$).</p>
            <aside class="notes">
              <p>A statistical model is essentially a randomized function that maps a set of predictor variables to an outcome variables.</p>
              <p>More precisely, you may call the shown model a regression model.</p>
              <p>However, consider for example your favorite phenotype (like certain disease or trait) and the human DNA. The human DNA consists of 3 billion base pairs, and one can't expect that all bases have an effect on the specific phenotype. In fact, one can expect that only the changes/mutations within a relatively small number of genes will have an effect. This brings us to the topic of sparse models.</p>
            </aside>
          </section>

          <section>
            <h3>Sparse Models</h2>
            <img width = 400 src="./img/20180711-Dissertation-Defense/sparsity.png" alt="The sparsity assumption in statistical modeling">
            <aside class="notes">
              <p>Here is an illustration of what it means to have a sparse regression model, where the distribution of the phenotype or outcome variable y conditional on the predictor variables x5, x8, and x13 is the same as the distribution conditional on all potential predictor variables x1, x2, and so on.</p>
            </aside>
          </section>

          <section data-background="#fff">
            <h3>The two-faced model selection problem</h3>
            <p style="text-align:left;">
              <i class="em em-arrow_right"></i> <strong>Prediction:</strong>
              <span style="float:right;">
                <i class="em em-arrow_right"></i> <strong>Feature selection:</strong>
              </span>
            </p>
            <p style="text-align:left;">
              <i>Find best predictions for $y$.</i>
              <span style="float:right;">
                <i>Which $x\subscript{j}$ are predictive?</i>
              </span>
            </p>
            <h3>Two types of false findings</h3>
            <p style="text-align:left;">
              <i class="em em-scream"></i> <strong>False positives.</strong>
              <span style="float:right;">
                <i class="em em-scream_cat"></i> <strong>False discoveries.</strong>
              </span>
            </p>
            <p style="text-align:left;">
              Overfitting.
              <span style="float:right;">
                Curse of dimensionality.
              </span>
            </p>
            <aside class="notes">
              <p>So, if there is this type of sparsity in the relationships between the input and output variables, then a statistical modeling actually has to solve two related but fundamentally different problems at the same time.</p>
              <p>One - obtain the best possible predictions of the outcome variables y based on the given predictor variables x1, x2, and so on; and two - find the subset of predictor variables that are actually associated with the outcome variable y, and dispose of the irrelevant variables.</p>
              <p>Both problems are essential in biomedical applications. One for example for disease diagnosis or risk prediction, and the other to help understand the cause of disease, that is, for the identification of targets for treatment.</p>
              <p>Depending on which of the two modeling problems you consider, you are dealing with a conceptionally different type of false findings.</p>
              <p>In prediction you may have FP (or FN), e.g., a misdiagnosis; while in feature selection we have false discoveries, e.g., a gene identified as associated to disease when it is not.</p>
              <p>Unfortunately, many ML methods, and also many statistical models, which are commonly used in genomics and neuroimaging (and seem to perform well), provide no way to obtain an idea of how many false findings to expect.</p>
            </aside>
          </section>

          <section data-background="#fff">
            <h2>Aims</h2>
            <p>Establish <strong>guarantees</strong> on...</p>
            <ul>
              <li class="fragment grow">false discoveries in feature selection,</li>
              <li class="fragment grow">false predictions on new data (generalization)</li>
            </ul>
            <p>...for types of methods commonly used in the analysis of genomic and neuroimaging data.</p>
            <aside class="notes">
              <p>In my thesis, I proposed methods similar to the ones widely used in genomics and neuroimaging. However, my methods focus on explicitly establishing guarantees...</p>
            </aside>
          </section>
        </section>

        <section>
          <section data-background="#003300">
            <h3>Resources and collaborators</h3>
            <ul>
              <li>The Multiscale Bioimaging and Bioinformatics Laboratory (MBB) at Tulane University.</li>
              <li>Tulane Center for Bioinformatics and Genomics (CBG).</li>
              <li>FDA, Office of Science and Engineering, Division of Imaging, Diagnostics, and Software Reliability.</li>
              <li>Other: The Mind Research Network, University of Wrocław, Indiana University Bloomington, University of Tennessee Health Science Center.</li>
            </ul>
            <aside class="notes">
              <p>At this point I would like to acknowledge my collaborators at Tulane at beyond...</p>
              <p>Most importantly Dr. Yu-Ping Wang's MBB lab at Tulane.</p>
              <p>And a few other places where people who have helped me with my research work.</p>
            </aside>
          </section>
        </section>

        <!-- Feature selection background -->
        <!-- Feature selection in regression problems -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/dna-with-equations.jpg">
            <div style="background-color: rgba(0, 0, 0, 0.8); color: #fff; padding: 20px;">
              <h3>Feature selection in genomics and neuroimaging</h3>
            </div>
            <aside class="notes">
              <p>First I will focus on the feature selection problem, and later I will talk about the prediction problem.</p>
              <p>Feature selection is important for accuracy of prediction (removing redundant or "noisy" features), inexpensive diagnosis (accurate prediction based on a small number of features), and data-generated biological hypotheses.</p>
            </aside>
          </section>

          <section>
            <h3>Multiple hypotheses testing</h3>
              <p>
              <strong>Feature selection</strong> as testing of hypotheses:
              $$H\subscript{i} : \beta\subscript{i} = 0, \quad i = 1,\ldots,p.$$
              </p>
              <ul>
                <li>$\beta\subscript{i} := $ effect of $i$th feature.</li>
                <li>$R := $ number of rejected hypotheses.</li>
                <li>$V := $ number of false rejections (i.e., Type I errors).</li>
                <li class="fragment highlight-green"><strong>Family-wise error rate:</strong> $\mathrm{FWER} = \mathbb{P}(V \geq 1)$.<sup>[1]</sup></li>
                <li class="fragment highlight-red"><strong>False discovery rate:</strong> $\mathrm{FDR} = \mathbb{E}\left( \frac{V}{\min\{R, 1\}} \right)$.<sup>[2]</sup></li>
              </ul>
              <br><br>
              <p><small>
                [1]: E.g., Bonferroni, Holm (1979), Hommel (1988).
                <br>
                [2]: E.g., Benjamini-Hochberg (1995), Benjamini-Yukutieli (2001).
              </small></p>
              <aside class="notes">
                <p>The classical way to avoid false discoveries is found in the theory on multiple hypotheses testing.</p>
                <p>There are two imporant measures of error rates in feature selection - FWER and FDR.</p>
                <p>By design the FDR or FWER will be controlled.  E.g, GWAS.</p>
                <p>I focus on FDR. It is more advantageous in ver high-dimensional setting, because FWER is too unforgiving and strict (FWER says "not a singly FD", FDR says "FDs are ok as long as their proportion is below a certain percentage" leading to higher detection power).</p>
                <p>But: Like trying to understand a sentence by analysing each letter separately, one at a time.</p>
              </aside>
          </section>

          <section>
            <h3>Sparse regression</h3>
            <p><strong>LASSO:</strong> $\hat{\boldsymbol{\beta}} = \arg\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert\mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \lambda\lVert\mathbf{b}\rVert\subscript{1}.$<small><sup>[1]</sup></small></p>
            <ul>
              <li>$\mathbf{y} = f(X) + \boldsymbol{\varepsilon} \approx f(X) = X\boldsymbol{\beta} \approx X\hat{\boldsymbol{\beta}}$.</li>
              <li>Yields a sparse solution $\hat{\boldsymbol{\beta}}$.</li>
              <li>Computationally efficient (convex).</li>
              <li>Very useful in practice.</li>
              <li class="fragment highlight grow"><strong>Problem:</strong> how sparse should $\hat{\boldsymbol{\beta}}$ be?</li>
              <li class="fragment highlight grow"><strong>Problem:</strong> how to do statistical inference on $\hat{\boldsymbol{\beta}}$?</li>
            </ul>
            <br>
            <p><small>
              [1]: Tibshirani, JRSSB, 1996.
            </small></p>
            <aside class="notes">
              <p>All features are analysed jointly (possible to include interaction effects, non-linear effects).</p>
              <p>It essentially amounts to the estimation of a coefficient vector beta, where each element may for example quantify the effect of a gene or brain region etc.</p>
              <p>Feature selection and prediction is performed simultaneously.</p>
              <p>A subset of significant features is selected from the whole set of features <i>in one step</i>, rather than "one letter at a time".</p>
              <p>Cannot control for the error rates mentioned on the last slide.</p>
            </aside>
          </section>
        </section>

        <!-- SLOPE and Group SLOPE -->
        <section>
          <section>
            <h4>Sorted L-One Penalized Estimation<sup>[1]</sup></h4>
            <p>$\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}} = \mathrm{argmin}\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \lVert \mathbf{y} - X\mathbf{b}\rVert\subscript{2}^2 + \sum\subscript{i=1}^p \lambda\subscript{i} |\mathbf{b}|\subscript{(i)},$</p>
            <p>where $\lambda\subscript{1} \geq \lambda\subscript{2} \geq \ldots \geq \lambda\subscript{p} \geq 0$; and $|b|\subscript{(1)} \geq |b|\subscript{(2)} \geq \ldots \geq |b|\subscript{(p)}$ denotes the order statistic of the magnitudes of the vector $\mathbf{b}\in\mathbb{R}^p$.</p>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> Given $q\in(0,1)$, there is a procedure to choose $\boldsymbol{\lambda}$ s.t. $\mathrm{FDR}(\hat{\boldsymbol{\beta}}\subscript{\mathrm{SLOPE}}) \leq q$ is guaranteed. <i class="fragment">...if the explanatory variables have very small pair-wise correlations.</i> <i class="fragment" style="color: #ff2c2d;"> ← typically not the case in genomics.<sup>[2]</sup></i></p>
            <p><small>[1]: Bogdan et. al., Annals Appl Stat, 2015. [2]: Gossmann et. al., ACM BCB, 2015.</small></p>
          </section>

          <section>
            <h2>Group SLOPE Motivation</h2>
            <ul>
              <li>Divide the data into groups by correlation. <i class="fragment" style="color: #17ff2e;"> ← Often possible for biological data.</i></li>
              <li class="fragment">Then select/drop entire groups rather than individual variables.</li>
              <li class="fragment">Redefine FDR w.r.t. groups: <strong>gFDR</strong>.</li>
            </ul>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group-wise false discovery rate

* $\mathrm{Rg} := $ "# groups discovered by Group SLOPE"
* $\mathrm{Vg} := $ "# falsely discovered groups"

We define
$$\mathrm{gFDR} := \mathrm{E} \left( \frac{\mathrm{Vg}}{\max(\mathrm{Rg}, 1)} \right).$$
            </script>
          </section>

          <section>
            <h2>Group SLOPE</h2>
            <ul>
              <li>$\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$, $X\in\mathbb{R}^{n\times p}$, $\boldsymbol{\beta}\in\mathbb{R}^p$, $\boldsymbol{\varepsilon}\sim\mathrm{N}(0, \sigma\subscript{\varepsilon}^2 I)$.</li>
              <li>$\boldsymbol{\beta}$ divided into $J$ groups of sizes $p_1, p_2, \cdots, p_J$, i.e. $\boldsymbol{\beta} = (\boldsymbol{\beta}_1^T, \boldsymbol{\beta}_2^T, \ldots, \boldsymbol{\beta}_J^T)^T$ with $\boldsymbol{\beta}_i \in \mathbb{R}^{p_i}$.</li>
            </ul>
            <p class="fragment fade-down"><i class="em em-arrow_down"></i>
            $$\min\subscript{\mathbf{b}\in\mathbb{R}^p} \frac{1}{2} \left\lVert\mathbf{y} - X\mathbf{b}\right\rVert\subscript{2}^2 + \sum\subscript{i=1}^J \lambda\subscript{i} \sqrt{p\subscript{(i)}}\left\lVert X\subscript{(i)} \mathbf{b}\subscript{(i)}\right\rVert\subscript{2},$$
            where $\sqrt{p\subscript{(1)}}\left\lVert  X\subscript{(1)} \mathbf{b}\subscript{(1)} \right\rVert\subscript{2} \geq \sqrt{p\subscript{(2)}}\left\lVert  X\subscript{(2)} \mathbf{b}\subscript{(2)} \right\rVert\subscript{2} \geq \ldots$</p>
          </section>

          <section style="text-align: left;">
            <strong>Different ways to find the global solution:</strong>
            <img width="455" align="right" src="./img/20180711-Dissertation-Defense/FISTA_vs_ADMM_runtimes.png" alt="Comparison of FISTA and ADMM optimizers for Group SLOPE">
            <li><i>Fast iterative shrinkage-thresholding algorithm (FISTA)</i> &mdash; a proximal gradient method used in [1-3].</li>
            <li><i>Alternating direction method of multipliers (ADMM)</i> &mdash; derived in the thesis.</li>
            <p align="right"><small>[Figure 3.2 in the thesis]</small></p>
            <p><small>
              [1]: Gossmann et. al., 2015. [2]: Brzyski, Gossmann, et. al., 2018. [3]: Gossmann et. al., 2017.
            </small></p>
            <aside class="notes">
              <p>Even if our method has some kind of superior statistical properties, it would be useless without a way to obtain the solution efficiently.</p>
              <p>In the thesis I derive two types of optimization routines.</p>
            </aside>
          </section>

          <section>
            <h3>Group SLOPE - Theoretical guarantees</h3>
            <p>Given a user-specified $q \in (0, 1)$, we show how to choose $\boldsymbol{\lambda}$ such that $\mathrm{gFDR} \leq q$.<sup>[1-3]</sup></p>
            <p><small><b>Different approaches:</b> theoretical for orthogonal designs<sup>[2-3]</sup>, heuristic based on theory for general designs<sup>[1-2]</sup>, Monte Carlo based for general designs<sup>[3].</sup></small></p>
            <p><span class="fragment"><i class="em em-arrow_right"></i> Confirmed with extensive simulation studies on synthetic and real data.<sup>[1-3]</sup></span></p>
            <br>
            <br>
            <p><small>
              [1]: Gossmann et. al., 2015. [2]: Brzyski, Gossmann, et. al., 2018. [3]: Gossmann et. al., 2017.
            </small></p>
          </section>

          <section data-markdown>
            <script type="text/template">
![Chromosome 22 SNPs, simulated phenotype](./img/20180711-Dissertation-Defense/gFDR_non-ortho.png)
<small> $X \in \mathbb{R}^{8915\times 5976}$ contains real DNA sequence data of chromosome 22; 726 groups (lengths from 1 to 1657, mean=$8.23$, median=1); between-group correlation < 0.3; **simulated response** $\mathbf{y} = X\boldsymbol{\beta} + \mathbf{z},$ where $\mathbf{z} \sim \mathcal{N}(0, I)$. [Figure 3.5 in the thesis] </small>
            </script>
          </section>

          <section>
            <h3>Application - Framingham Cohort Analysis<sup>[1]</sup></h3>
            <ul>
              <li>SNP data for 8915 subjects.</li>
              <li>1771 subjects have corresponding spine BMD measurements.</li>
              <li>The remaining ~7000 subjects used to group SNPs.</li>
            </ul>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> $X$ with dimensions $1771 \times 117933$, consisting of 6403 groups of average size 18.42 (median size 2).</p>
            <br>
            <p><small>
              [1]: Gossmann et. al., 2017.
            </small></p>
          </section>

          <section data-markdown>
            <script type="text/template">
### Group SLOPE results

* 40 SNPs were selected by Group SLOPE with target gFDR $q = 0.1$, and mapped to nearby genes.
* 15 genes reported in previous studies:
  - BMD (SMOC1, RPS6KA5, FGFR2, GAA, SCN1A, RAB5A, SOX1, and A2BP1),
  - osteoarthritis (A2BP1, ADAM12, MATN1),
  - lumbar disc herniation (KIAA1217),
  - osteopetrosis (VAV3),
  - biology of osteoclasts, osteoblasts and osteogenesis (VAV3, SLC7A7, ADAM12, PPARD, FGFR2, PTPRU, SMOC1).
            </script>
          </section>

          <section>
            <h3>Group SLOPE</h3>
            <p><b>Further topics covered in the thesis but skipped here:</b></p>
            <ul>
              <li>Performance of SLOPE on DNA seq data.<sup>[1]</sup></li>
              <li>An alternative formulation of Group SLOPE.<sup>[1, 3]</sup></li>
              <li>Theoretical gFDR control under orthogonal groups.<sup>[2-3]</sup></li>
              <li>Error variance estimation: <i>Scaled sparse linear regression</i> vs. <i>EigenPrism</i>.</li>
              <li>Analysis of runtime on large DNA seq datasets.<sup>[3]</sup></li>
            </ul>
            <br>
            <p><small>
              [1]: Gossmann et. al., 2015. [2]: Brzyski, Gossmann, et. al., 2018. [3]: Gossmann et. al., 2017.
            </small></p>
          </section>
        </section>

        <!-- CCA Background -->
        <section>
          <section data-background="#71C5E8">
            <h3>Canonical Correlation Analysis</h3>
            <img width=960 src="./img/20180711-Dissertation-Defense/SCCA.png" alt="(Sparse) Canonical Correlation Analysis">
            <aside class="notes">
              <p>In the previous model we had an out come variables y and many predictor variables represented by a high-dimensional vector x. But what if we y is high-dimensional too? For example y may represent a subject's fMRI acitvations per brain voxel, while x represents expression values per gene.</p>
            </aside>
          </section>

          <section>
            <h3>Classical CCA [Hotelling, 1936]</h3>
            <p>Consider two matrices (i.e., datasets)<br>$X\in\mathbb{R}^{n\times p}$ and $Y\in\mathbb{R}^{n \times q}$ (with columns centered).</p>
            <p>$$\mathrm{maximize}\subscript{u\in\mathbb{R}^p, v\in\mathbb{R}^q} \widehat{\mathrm{Cov}}(Xu, Yv) = \frac{1}{n} u^T X^T Y v,$$
            $$\mathrm{subject\,to} \quad \widehat{\mathrm{Var}}(Xu) = 1, \widehat{\mathrm{Var}}(Yv) = 1.$$ </p>
            <p class="fragment grow">The problem is degenerate if $n \leq \mathrm{max}\left( p, q \right)$.<br>+ Sparsity assumption on biological data</p>
          </section>

          <section>
            <h3>Sparse CCA<sup>[1-2]</sup></h3>
            <p>$\mathrm{maximize}\subscript{\mathbf{u}\in\mathbb{R}^p, \mathbf{v}\in\mathbb{R}^q} \frac{1}{n} \mathbf{u}^T X^T Y \mathbf{v},$</p>
            <p>subject to</p>
            <p>$\lVert \mathbf{u} \rVert_2^2 \leq 1, \lVert \mathbf{v} \rVert_2^2 \leq 1, \lVert \mathbf{u} \rVert_1 \leq c_1, \lVert \mathbf{v} \rVert_1 \leq c_2$.</p>
            <ul>
              <li>Unique solution even when $p_X, p_Y \gg n$.</li>
              <li class="fragment grow">Selection of the sparsity parameters remains a challenging problem.</li>
            </ul>
            <br><br>
            <p><small>[1]: Witten et. al., 2009, [2]: Parkhomenko et. al., 2009.</small></p>
          </section>
        </section>

        <!-- Sparse CCA with FDR control -->
        <section>
          <section>
            <h3>Sparse CCA</h3>
            <p><i class="em em-arrow_right"></i> Select sparsity parameters in a data-driven fashion, such that FDR is controlled.</p>
          </section>

          <section>
            <h3>Defining FDR for sparse CCA</h3>
            <ul>
              <li>Adapt the widely-used FDR concept from multiple hypotheses testing to CCA.<sup>[1]</sup>
                <blockquote>
                The expected proportion of &ldquo;discoveries&rdquo; that are false.
                </blockquote>
              </li>
              <li>Consider FDR in $\mathbf{u}$ and in $\mathbf{v}$ separately.</li>
            </ul>
            <br>
            <br>
            <p><small>[1]: Benjamini &amp; Hochberg, JRSSB, 1995.</small></p>
          </section>

          <section>
            <h3>Defining FDR for sparse CCA</h3>
            <blockquote>
              The coefficient estimate $\hat{u}_i \neq 0$ represents a <b>false discovery</b> of the $i$th feature of $X$, if $u_i$ doesn't affect the value of $\mathrm{Cov}(\mathbf{x} \cdot \mathbf{u}, \mathbf{y} \cdot \mathbf{v})$, or <i>equivalently</i> if
              $$\hat{u}_i \neq 0 \quad \text{and} \quad \mathrm{E}(X^T Y \mathbf{v})_i = 0.$$
            </blockqoute>
          </section>

          <section>
            <h3>Defining FDR for sparse CCA</h3>
            <ul>
              <li>$\mathrm{R}\subscript{\hat{\mathbf{u}}} =$ the number of non-zero elements in $\hat{\mathbf{u}}$,</li>
              <li>$\mathrm{V}\subscript{\hat{\mathbf{u}}} =$ the number of false discoveries.</li>
            </ul>
            <p>Define</p>
            <p>$\mathrm{FDR}(\hat{\mathbf{u}}) := \mathbb{E}\left( \frac{\mathrm{V}\subscript{\hat{\mathbf{u}}}}{\max\left\{ \mathrm{R}\subscript{\hat{\mathbf{u}}}, 1 \right\}} \right).$</p>
          </section>

          <section>
            <h3>Defining gFDR for sparse CCA</h3>
            <p>Analogously we define the group-wise false discovery rate (gFDR).</p>
            <ul>
              <li>$\mathrm{Rg}\subscript{\hat{\mathbf{u}}} =$ the number of non-zero groups of elements in $\hat{\mathbf{u}}$,</li>
              <li>$\mathrm{Vg}\subscript{\hat{\mathbf{u}}} =$ the number of falsely discovered groups.</li>
            </ul>
            <p>Define</p>
            <p>$\mathrm{gFDR}(\hat{\mathbf{u}}) := \mathbb{E}\left( \frac{\mathrm{Vg}\subscript{\hat{\mathbf{u}}}}{\max\left\{ \mathrm{Rg}\subscript{\hat{\mathbf{u}}}, 1 \right\}} \right).$</p>
          </section>
        </section>

        <!-- SlopeCCA and gSlopeCCA -->
        <section>
          <section data-background="#D0DEBB">
            <h1>SlopeCCA and gSlopeCCA</h1>
            <aside class="notes">
              <p>Having the FDR and gFDR definition in place, we propose three different sparse CCA methods that keep the FDR or gFDR below a user-specified level.</p>
            </aside>
          </section>

          <section>
            <p><strong>slopeCCA:</strong></p>
            <p><small>
              $\mathrm{minimize}\subscript{\fatu\in\R^p, \fatv\in\R^q}\, \left\{ -\fatu^T X^T Y \fatv + \sqrt{n} J\subscript{\fatlambda^u}(\fatu) + \sqrt{n} J\subscript{\fatlambda^v}(\fatv) \right\}$,
              <br>
              subject to $\lVert\fatu\rVert\subscript{2}^2 \leq 1, \lVert\fatv\rVert\subscript{2}^2 \leq 1$.
            </small></p>
            <p><strong>gslopeCCA:</strong></p>
            <p><small>
              $\mathrm{minimize}\subscript{\fatu\in\R^p, \fatv\in\R^q} \left\{ -\fatu^T X^T Y \fatv + \sqrt{n} J\subscript{\fatlambda^u}\left( \left( \lVert \fatu\subscript{1} \rVert\subscript{2}, \dots\right)^T \right) + \sqrt{n} J\subscript{\fatlambda^v}\left(  \left( \lVert \fatv\subscript{1} \rVert\subscript{2}, \dots \right)^T \right) \right\}$,
              subject to $\lVert\fatu\rVert\subscript{2}^2 \leq 1, \lVert\fatv\rVert\subscript{2}^2 \leq 1$.
            </small></p>
            <p><small>Where $J\subscript{\fatlambda}(\fatu) = \Sigma\subscript{i=1}^p \lambda\subscript{i} |u|\subscript{(i)}$ is the Sorted L1 Norm.</small></p>
          </section>

          <section>
            <h3>SlopeCCA and gslopeCCA - computational methods</h3>
            <p>Both are <strong>biconvex optimization problems</strong>.</p>
            <p><strong>Alternating optimization</strong> algorithms are guaranteed to converge.</p>
            <aside class="notes">
              <p>Again, for the methods to be useful in practice, we need to know how to obtain the solution efficiently.</p>
            </aside>
          </section>

          <section>
            <h3>SlopeCCA and gSlopeCCA - theoretical guarantees</h3>
            <p>Asymptotic (i.e., as $n\to\infty$) FDR and gFDR guarantees if $Cov(X)$ and $Cov(Y)$ are diagonal or block-diagonal.</p>
            <p>($Cov(X, Y)$ can be of arbitrary shape)</p>
          </section>

          <section>
            <img width="800" src="./img/20180711-Dissertation-Defense/slopeCCA_FDR.png" alt="Simulation studies investigating the false discovery rates resulting from slopeCCA, gslopeCCA, and L1-penalized sparse CCA.">
            <p><small><strong>Simulation studies:</strong> FDR and gFDR of slopeCCA, gslopeCCA, and $\ell_1$-penalized CCA solutions; $n = 200, 1000, 5000, 10000$ and $p_X = p_Y = 300$; 11 evenly spaced sparsity levels between 0 (i.e., $X$ is uncorrelated with $Y$) and 1 (i.e., every feature of $X$ is correlated to some feature of $Y$, and vice versa); 500 independent simulation runs; dashed line represents theoretical bounds for slopeCCA and gslopeCCA.<br>[Figure 4.1 in the thesis]</small></p>
          </section>

          <section data-markdown>
            <script type="text/template">
### Application to TCGA

The Cancer Genome Atlas (TCGA):

* NIH initiative since 2005.
* Coordinated data collection at 20 collaborating institutions in U.S. and Canada.
* Genomic samples from tumor cells of different cancers and matched normal cells.
* Data include: gene expression, methylation, CNV, SNP, microRNA, and whole genome, exon, or transcriptome sequencing.
* <https://cancergenome.nih.gov/>
            </script>
          </section>

          <section>
            <img width="700" src="./img/20180711-Dissertation-Defense/TCGA_gslopeCCA_canonical_variates.png" alt="Results from an application of gSlopeCCA to the Cancer Genome Atlas data">
            <p><small>Canonical variates estimated by gslopeCCA on methylation and mRNA data from TCGA ($n = 1109$, $p_X = 24981$, $p_Y = 20255$).
              Observations are colored according to cancer type, revealing clear differences.
              <br>[Figure 4.2 in the thesis]</small></p>
            <aside class="notes">
              <p>Apart from looking at the results of feature selection (i.e., which genes got selected),
              we can also look at the canonical variates.
              These derived features can reveal new information about disease phenotypes,
              and can be used for dimensionality reduction, classification, and prediction.
              Here we see that by looking at a pair of canonical variates (only two features!)
              one can distinguish different types of cancer cells rather well.</p>
            </aside>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### TCGA &mdash; Conclusions

* Canonical variates reveal differences between cancer types.
* Using the four canonical variates as the only predictors to classify cancer type yields classification accuracy over 93% on test data.
* The estimated canonical vectors are not sparse (over 95% of groups selected). Biological interpretation not feasible.
            </script>
          </section>
        </section>

        <!-- FDR-corrected SCCA -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/T2star_preproc_facetted.gif">
            <h1>FDR-corrected sparse CCA</h1>
          </section>

          <section>
            <h2>FDR-corrected sparse CCA</h2>

            <p><b>Motivation:</b> The assumption of slopeCCA and gslopeCCA that $Cov(X)$ and $Cov(Y)$ are diagonal or block-diagonal is too restrictive in many cases.</p>
          </section>

          <section>
            <h2>FDR-corrected sparse CCA</h2>
            <p>A split-sample, two-step procedure:<sup>[1]</sup></p>
            <ol>
              <li>Split the data in two parts.</li>
              <li><strong>Using the first subsample:</strong> obtain initial estimates $\hat{\fatu}^{(0)}$ and $\hat{\fatv}^{(0)}$ using conventional sparse CCA.</li>
              <li><strong>Using the second subsample:</strong> test hypotheses of the form,
                $$\mathrm{H}^{(u)}\subscript{i} : \mathrm{E}\left(X^T Y \mathbf{v}\right)\subscript{i} = 0, \quad \mathrm{H}^{(v)}\subscript{j} : \mathrm{E}\left(Y^T X \mathbf{u}\right)\subscript{j} = 0,$$
                and adjust for multiple comparisons to control FDR.
              </li>
            </ol>
            <br>
            <br>
            <small>[1]: Gossmann et. al., IEEE TMI, 2018.</small></p>
            <aside class="notes">
              I modified my approach.
            </aside>
          </section>

          <section>
            <h3>FDR-corrected sparse CCA - Theoretical FDR Guarantees</h3>
            <ol>
              <li>Asymptotic theory allows us to approximate the distributions of $X^T Y \hat{\fatv}^{(0)}$ and $Y^T X \hat{\fatu}^{(0)}$.</li>
              <li>We can use the Benjamini-Hochberg procedure to control the FDR.</li>
            </ol>
            <p class="fragment"><i class="em em-arrow_right"></i> FDR control confirmed with extensive simulation studies on synthetic and real data.<sup>[1]</sup>
            <br>
            <br>
            <small>[1]: Gossmann et. al., IEEE TMI, 2018.</small></p>
          </section>

          <section>
            <h3>Simulation Studies</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/high_dim_FDR_combined.png" alt="Simulation studies of the FDR-corrected sparse CCA method.">
            <p>$X, Y \in \R^{600 \times 1500}$.
            FDR is controlled regardless of the sparsity of the true solution.</p>
          </section>

          <section>
            <h3>Simulation Studies</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/high_dim_TPR_combined.png" alt="Simulation studies of the FDR-corrected sparse CCA method.">
            <p>True positive rate is on par with competing methods.</p>
          </section>

          <section>
            <h3>FDR-corrected SCCA - Application</h3>
            <ul>
              <li>Diversity in brain activity and brain connectivity in children and adolescents.</li>
              <li>What are the driver genes?</li>
              <li>Relationship to neurodevelopmental and psychiatric disorders.</li>
            </ul>
          </section>
          <section>
            <h3>Dataset</h3>
            <blockquote>
              The Philadelphia Neurodevelopmental Cohort (PNC) is a large-scale collaborative study between the Brain Behaviour Laboratory at the University of Pennsylvania and the Children's Hospital of Philadelphia. It contains a fractal $n$-back fMRI task, an emotion identification fMRI task, SNP arrays, and questionnaire data for over 900 adolescents.
            </blockquote>
            <aside class="notes">
              <p>Including subjects with increased (prodromal) symptoms of ADD (107 PNC subjects), schizophrenia (103 PNC subjects), and depression (85 PNC subjects) [Kaufmann et. al., 2017].</p>
            </aside>
          </section>
          <section>
            <h3>Objective</h3>

            <p>Use sparse CCA to identify the relationships between brain activity, brain connectivity, and genomics.</p>
          </section>
          <section>
            <h3>Application 1: n-back fMRI vs. SNPs</h3>
            <img width="960" src="./img/20180711-Dissertation-Defense/voxels_vs_genes.png" alt="PNC results: n-back task fMRI vs. SNP analysis">
            <p><small>Selection from $85796$ brain voxels and $60372$ genomic features.</small></p>
          </section>
          <section>
            <h4>Results validation - n-back fMRI vs. SNPs</h4>
            <img width="260" src="./img/20180711-Dissertation-Defense/voxels_vs_genes.png" alt="PNC results: n-back task fMRI vs. SNP analysis">
            <ol>
              <li>Similar brain regions have been found in other fMRI studies of working memory.</li>
              <li>At least 34 out of the 65 identified genes have been previously associated with various aspects of human cognitive function.</li>
            </ol>
          </section>
          <section>
            <h3>Application 2: Functional connectivity (FC) vs. SNPs</h3>
            <ol>
              <li>Emotion identification task fMRI data transformed to FC measures.</li>
              <li>FDR-corrected sparse CCA solution includes 129 genomic features and 107 FC features.</li>
            </ol>
          </section>
          <section>
            <h4>FC vs. SNPs - Top 10 selected genes</h4>
            <small>
            <table>
              <thead>
                <tr>
                  <th>Gene</th>
                  <th>Previously studies in association with...</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>DAB1   </td>
                  <td>Autism, schizophrenia, brain development</td>
                </tr>
                <tr>
                  <td>NAV2   </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>WWOX   </td>
                  <td>Cognitive ability, brain development</td>
                </tr>
                <tr>
                  <td>CNTNAP2</td>
                  <td>Autism, brain connectivity, brain development, schizophrenia, major depression, cognitive ability (linguistic processing)</td>
                </tr>
                <tr>
                  <td>NELL1  </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>PTPRT  </td>
                  <td>Brain development</td>
                </tr>
                <tr>
                  <td>FHIT   </td>
                  <td>Cognitive ability, autism, ADHD</td>
                </tr>
                <tr>
                  <td>MACROD2</td>
                  <td>Autism</td>
                </tr>
                <tr>
                  <td>LRP1B  </td>
                  <td>Cognitive function</td>
                </tr>
                <tr>
                  <td>DGKB   </td>
                  <td>Brain development, bipolar disorder</td>
                </tr>
              </tbody>
            </table>
            </small>
            <p><small>(for detail see [Gossmann et. al., TMI, 2018])</small></p>
          </section>
        </section>

        <!-- Thresholdout -->
        <section>
          <section data-background="#78BE20">
            <h3>Another type of false findings</h3>
            <p><i class="em em-white_check_mark"></i> Feature selection with FDR control.</p>
            <p><i class="em em-arrow_right"></i> Features can be used to fit a predictive model.</p>
            <p class="fragment grow">Danger of over-fitting to the local noise in the given dataset, resulting in false predictions on new data.</p>
            <p>What to do?</p>
          </section>

          <section>
            <p>In Machine Learning practice, generally, usage of two independent datasets &mdash; <i>"training"</i> and <i>"test"</i> data.</p>
            <p class="fragment"><strong>Training data:</strong> exploratory analysis, model fitting, parameter tuning, comparison of different machine learning algorithms, feature selection, etc.</p>
            <p class="fragment fade-right">$\Longrightarrow$ Adaptive machine learning, risk of overfitting.</p>
            <p class="fragment"><strong>Test data:</strong> Performance evaluation <i>after the trained machine learning algorithm has been "frozen"</i>.</p>
            <p class="fragment fade-right">$\Longrightarrow$ Accurate performance measures of the final model, <strong>if the test data is used only once</strong>.</p>
              <aside class="notes">
              The performance metric obtained on test data will make it evident whether any overfitting has occurred during model training. Other techniques (multiple testing, cross-validation, bootstrap) can be used to avoid or reduce the overfitting on the training data.
              </aside>
          </section>

          <section data-transition="slide-in fade-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_1.png" alt="Test data reuse 1">
          </section>

          <section data-transition="fade-in fade-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_2.png" alt="Test data reuse 2">
          </section>

          <section data-transition="fade-in slide-out">
            <img width="960" src="./img/20180211-SPIE-Medical-Imaging/test_data_reuse_3.png" alt="Test data reuse 3">
          </section>

          <section>
            <h2>Idea</h2>
            <p>Can we <strong>obfuscate</strong> the test data to avoid overfitting?</p>
            <p class="fragment fade-right"><i class="em em-arrow_right"></i> Differential privacy.<sup>[1]</sup></p>
            <p class="fragment">Promising simulation results.<sup>[2-3]</sup>
            <br><br><br>
            <small>[1]: Dwork, McSherry, Nissim, Smith, 2006.<br> [2]: Dwork et. al., Science, 2015.<br> [3]: Gossmann et. al., SPIE 2018.</small></p>
          </section>

          <section>
            <h3>Differential privacy (Dwork, McSherry, Nissim, Smith, 2006)</h3>

            <ul>
              <li>A mathematically rigorous definition of data privacy.
                  $$P[\mathcal{M}(D) \in S] \leq e^\varepsilon P[\mathcal{M}(D^\prime) \in S] + \delta.$$
              </li>
              <li><b>Intuition:</b> An individual data point has little impact on the value reported by a DP mechanism.</li>
              <li><b>Intuition:</b> An adversary cannot learn an individual data point from querying a DP mechanism.</li>
              <li><b>Properties:</b> DP is <i>preserved</i> under <i>post-processing</i> and under <i>adaptive composition</i>.</li>
            </ul>
          </section>

          <section data-markdown>
            <script type="text/template">
### Differentially private access to test data

* Currently available literature focuses on theory.
* Available theoretical requirements too restrictive for most of applied data analysis and machine learning.
* Computational experiments available in the literature consider only simple instances of adaptivity, simple performance metrics, and simple machine learning algorithms &mdash; not capturing the reality of current data analysis practices adequately.
            </script>
          </section>

          <section style="text-align: left;">
<img src="./img/20180211-SPIE-Medical-Imaging/ROC.png" align="right" alt="Figure of an ROC curve">
<li>Invariance to prevalence.</li>
<li>Independence of the decision threshold (can be chosen later).</li>
<li>Probabilistic meaning.</li>
<li>Extensively used in the medical field, including medical imaging.</li>
          </section>

          <section data-markdown>
            <script type="text/template">
            ### Thresholdout + AUC = <i class="em em-heart"></i>

            __Thresholdout$_{\mathrm{AUC}}$__ combines the original __Thresholdout (DFHPRR, Science, 2015)__ with __AUC__ as the reported performance metric on test data.
            </script>
          </section>

          <section>
            <h3>Thresholdout$_{\mathrm{AUC}}$ &mdash; rough summary</h3>

            <div align="center">
              <div style='background-color: rgba(0, 0, 0, 0.9); width: 600px;'>
                <p style='border:4px black solid; font-size:42px;'>
                  Trained classifier $\phi(x) \in [0,1]$
                </p>
              </div>
            </div>

            <p><i class="em em-arrow_down"></i></p>

            <div align="center">
              <div style='background-color: rgba(0, 0, 0, 0.9); width: 800px;'>
                <div align="left">
                  <p style='border:4px black solid; font-size:42px;'>
                  <b>If</b> $\lvert \mathrm{AUC}_{\mathrm{training}}(\phi) - \mathrm{AUC}_{\mathrm{test}}(\phi) \rvert > \tilde{T}$:<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;output $\mathrm{AUC}_{\mathrm{test}}(\phi) +$ "a little noise"<br>
                  <b>Else</b>:<br>
                  &nbsp;&nbsp;&nbsp;&nbsp;output $\mathrm{AUC}_{\mathrm{training}}(\phi)$
                  </p>
                </div>
              </div>
            </div>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### Theorem &mdash; rough summary

              1. **If** a test dataset, which is used for performance evaluation repeatedly, is only accessed via Thresholdout$_{\mathrm{AUC}}$.
              $\Longrightarrow$ **Then** with a high probability $(1 − \beta)$ the reported AUC estimates will be correct up to a small tolerance $\tau$.
              2. **Restriction**: Test data access "budget" $B$, which is linear in the size of the test data $n$, and also depends on $\beta$, $\tau$, and the class balance.

              <aside class="notes">
              * $\beta$ and $\tau$ are pre-specified by the analyst.
              * This result holds even when each analysis step is adaptively chosen based on the reported AUC estimates obtained from previous analyses on the same test data using Thresholdout$_{\mathrm{AUC}}$.
              </aside>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### Logistic Regression

              ![Simulation results](./img/20180211-SPIE-Medical-Imaging/naive_holdout_reuse_vs_thresholdout_AUC_landscape_GLM_only.png)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### Logistic Regression

              * __Naive approach__: Classifier learns the effect of local noise in the test data (overfitting).
              * __Thresholdout approach__: The gap between the reported and the true AUC is much narrower!

              ![Simulation results](./img/20180211-SPIE-Medical-Imaging/naive_holdout_reuse_vs_thresholdout_AUC_landscape_GLM_only_small.png)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
              ![Simulation results](./img/20180211-SPIE-Medical-Imaging/naive_holdout_reuse_vs_thresholdout_AUC_landscape.png)
              Accuracy of reported AUC values is improved, at the cost of slightly higher uncertainty in the reported AUC, and slightly worse predictive performance.
            </script>
          </section>
        </section>

        <section>
          <section data-background="#093748">
            <p><strong>Parts of this work appear in:</strong></p>
            <ol>
              <li><strong>G.A.</strong>, Cao, S., &amp; Wang, Y.-P. In proceedings of ACM BCB '15. 2015.</li>
              <li><strong>G.A.</strong>, Cao, S., Brzyski, D., Zhao, L. J., Deng, H. W., &amp; Wang, Y. P. IEEE/ACM TCBB. 2017.</li>
              <li>Brzyski, D., <strong>G.A.</strong>, Su, W., &amp; Bogdan, M. JASA. 2018.</li>
              <li><strong>G.A.</strong>, Zille, P., Calhoun, V., &amp; Wang, Y.-P. IEEE TMI. 2018.</li>
              <li><strong>G.A.</strong>, Pezeshk, A., &amp; Sahiner, B. In proceedings of SPIE Medical Imaging '18. 2018.</li>
            </ol>
          </section>
        </section>

        <!-- Last slide -->
        <section>
          <section data-background="./img/20180711-Dissertation-Defense/dna-with-equations.jpg">
            <img width="600" src="./img/20180711-Dissertation-Defense/T2star_preproc_facetted_questions.gif" alt="Structural MRI animation">
          </section>
        </section>

      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>

      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // MathJax integration
        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'
        },

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/search/search.js', async: true },
          // Zoom in and out with Alt+click
          { src: 'plugin/zoom-js/zoom.js', async: true },
          // Speaker notes
          { src: 'plugin/notes/notes.js', async: true },
          // MathJax
          { src: 'plugin/math/math.js', async: true }
        ]
      });

    </script>

  </body>
</html>
